{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d07c2e-1f0b-44d9-9e99-d0a3485b52fa",
   "metadata": {},
   "source": [
    "[트랜스포머를 활용한 자연어 처리] 목차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85aa63-68d0-4ea9-82da-a56c098f8203",
   "metadata": {},
   "source": [
    "# Chapter 1. 트랜스포머 소개\n",
    "\n",
    "## 1.1 인코더-디코더 프레임워크\n",
    "\n",
    "## 1.2 어텐션 메커니즘\n",
    "\n",
    "## 1.3 NLP의 전이 학습\n",
    "\n",
    "## 1.4 허깅페이스 트랜스포머스\n",
    "\n",
    "## 1.5 트랜스포머 애플리케이션 둘러보기\n",
    "\n",
    "### 1.5.1 텍스트 분류\n",
    "\n",
    "### 1.5.2 개체명 인식\n",
    "\n",
    "### 1.5.3 질문 답변\n",
    "\n",
    "### 1.5.4 요약\n",
    "\n",
    "### 1.5.5 번역\n",
    "\n",
    "### 1.5.6 텍스트 생성\n",
    "\n",
    "## 1.6 허깅페이스 생태계\n",
    "\n",
    "### 1.6.1 허깅페이스 허브\n",
    "\n",
    "### 1.6.2 허깅페이스 토크나이저\n",
    "\n",
    "### 1.6.3 허깅페이스 데이터셋\n",
    "\n",
    "### 1.6.4 허깅페이스 액셀러레이트\n",
    "\n",
    "## 1.7 트랜스포머의 주요 도전 과제\n",
    "\n",
    "## 1.8 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63774b4-7a3d-4010-929e-e228af5c2267",
   "metadata": {},
   "source": [
    "# Chapter 2. 텍스트 분류\n",
    "\n",
    "## 2.1 데이터셋\n",
    "\n",
    "### 2.1.1 허깅페이스 데이터셋 처음 사용하기\n",
    "\n",
    "### 2.1.2 데이터셋에서 데이터프레임으로 \n",
    "\n",
    "### 2.1.3 클래스 분포 살펴보기\n",
    "\n",
    "### 2.1.4 트윗 길이 확인\n",
    "\n",
    "\n",
    "## 2.2 텍스트에서 토큰으로\n",
    "\n",
    "### 2.2.1 문자 토큰화\n",
    "\n",
    "### 2.2.2 단어 토큰화\n",
    "\n",
    "### 2.2.3 부분단어 토큰화\n",
    "\n",
    "### 2.2.4 전체 데이터셋 토큰화하기\n",
    "\n",
    "\n",
    "## 2.3 텍스트 분류 모델 훈련하기\n",
    "\n",
    "### 2.3.1 트랜스포머를 특성 추출기로 사용하기\n",
    "\n",
    "### 2.3.2 트랜스포머 미세튜닝하기\n",
    "\n",
    "\n",
    "## 2.4 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d20bd6-3152-41f1-b156-72e60ac36eda",
   "metadata": {},
   "source": [
    "# Chapter 3. 트랜스포머 파헤치기\n",
    "\n",
    "## 3.1 트랜스포머 아키텍처\n",
    "\n",
    "## 3.2 인코더\n",
    "\n",
    "### 3.2.1 셀프 어텐션\n",
    "\n",
    "### 3.2.2 피드 포워드 층\n",
    "\n",
    "### 3.2.3 층 정규화 추가하기\n",
    "\n",
    "## 3.3 디코더\n",
    "\n",
    "## 3.4 트랜스포머 유니버스\n",
    "\n",
    "### 3.4.1 트랜스포머 가계도\n",
    "\n",
    "### 3.4.2 인코더 유형\n",
    "\n",
    "### 3.4.3 디코더 유형\n",
    "\n",
    "### 3.4.4 인코더-디코더 유형\n",
    "\n",
    "## 3.5 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c4fd6-582c-427d-b37c-6b07f6782844",
   "metadata": {},
   "source": [
    "# Chapter 4. 다중 언어 개체명 인식\n",
    "\n",
    "## 4.1 데이터셋\n",
    "\n",
    "## 4.2 다중 언어 트랜스포머\n",
    "\n",
    "## 4.3 XLM-R 토큰화 \n",
    "\n",
    "### 4.3.1 토큰화 파이프라인\n",
    "\n",
    "### 4.3.2 SentencePiece 토크나이저\n",
    "\n",
    "## 4.4 개체명 인식을 위한 트랜스포머\n",
    "\n",
    "## 4.5 트랜스포머 모델 클래스\n",
    "\n",
    "### 4.5.1 바디와 헤드\n",
    "\n",
    "### 4.5.2 토큰 분류를 위한 사용자 정의 모델 만들기\n",
    "\n",
    "### 4.5.3 사용자 정의 모델 로드하기\n",
    "\n",
    "## 4.6 NER 작업을 위해 텍스트 토큰화 하기 \n",
    "\n",
    "## 4.7 성능 측정\n",
    "\n",
    "## 4.8 XLM-RoBERTa 미세 튜닝하기\n",
    "\n",
    "## 4.9 오류 분석\n",
    "\n",
    "## 4.10 교차 언어 전이\n",
    "\n",
    "### 4.10.1 제로샷 전이가 유용할 때\n",
    "\n",
    "### 4.10.2 다국어에서 동시에 미세 튜닝하기\n",
    "\n",
    "## 4.11 모델 위젯 사용하기 \n",
    "\n",
    "## 4.12 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a36912e-67fd-44b5-9c70-9dbe952f8f15",
   "metadata": {},
   "source": [
    "# Chapter 5. 텍스트 생성\n",
    "\n",
    "## 5.1 일관성 있는 텍스트 생성의 어려움\n",
    "\n",
    "## 5.2 그리디 서치 디코딩\n",
    "\n",
    "## 5.3 빔 서치 디코딩\n",
    "\n",
    "## 5.4 샘플링 방법\n",
    "\n",
    "## 5.5 Top-k 및 뉴클리어스 샘플링\n",
    "\n",
    "## 5.6 어떤 디코딩 방법이 최선일까요?\n",
    "\n",
    "## 5.7 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631c497-2eee-458a-b97e-8c25c188ee25",
   "metadata": {},
   "source": [
    "# Chapter 6. 요약\n",
    "\n",
    "## 6.1 CNN/DailyMail 데이터 셋\n",
    "\n",
    "## 6.2 텍스트 요약 파이프라인\n",
    "\n",
    "### 6.2.1 요약 기준 모델\n",
    "\n",
    "### 6.2.2 GPT-2\n",
    "\n",
    "### 6.2.3 T5\n",
    "\n",
    "### 6.2.4 BART\n",
    "\n",
    "### 6.2.5 PEGASUS\n",
    "\n",
    "## 6.3 요약 결과 비교하기\n",
    "\n",
    "## 6.4 생성된 텍스트 품질 평가하기 \n",
    "\n",
    "### 6.4.1 BLEU\n",
    "\n",
    "### 6.4.2 ROUGE\n",
    "\n",
    "## 6.5 CNN/DailyMail 데이터셋에서 PEGASUS 평가하기\n",
    "\n",
    "## 6.6 요약 모델 훈련하기 \n",
    "\n",
    "### 6.6.1 SAMSum에서 PEGASUS 평가하기\n",
    "\n",
    "### 6.6.2 PEGASUS 미세 튜닝하기\n",
    "\n",
    "### 6.6.3 대화 요약 생성하기\n",
    "\n",
    "## 6.7 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd247650-9d71-469c-93d5-341cbed443a1",
   "metadata": {},
   "source": [
    "# Chapter 7. 질문 답변\n",
    "\n",
    "## 7.1 리뷰 기반 QA 시스템 구축하기\n",
    "\n",
    "### 7.1.1 데이터 셋\n",
    "\n",
    "### 7.1.2 텍스트에서 답 추출하기\n",
    "\n",
    "### 7.1.3 헤이스택을 사용해 QA 파이프라인 구축하기\n",
    "\n",
    "## 7.2 QA 파이프라인 개선하기 \n",
    "\n",
    "### 7.2.1 리트리버 평가하기\n",
    "\n",
    "### 7.2.2 리더 평가하기\n",
    "\n",
    "### 7.2.3 도메인 적응\n",
    "\n",
    "### 7.2.4 전체 QA 파이프라인 평가하기 \n",
    "\n",
    "## 7.3 추출적 QA를 넘어서\n",
    "\n",
    "## 7.4 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3e54d-20af-4e6b-b088-b1e481165553",
   "metadata": {},
   "source": [
    "# Chapter 8. 효율적인 트랜스포머 구축\n",
    "\n",
    "## 8.1 의도 탐지 예제\n",
    "\n",
    "## 8.2 벤치마크 클래스 만들기\n",
    "\n",
    "## 8.3 지식 정제로 모델 크기 줄이기\n",
    "\n",
    "### 8.3.1 미세 튜닝에서의 지식 정제\n",
    "\n",
    "### 8.3.2 사전 훈련에서의 지식 정제\n",
    "\n",
    "### 8.3.3 지식 정제 트레이너 만들기\n",
    "\n",
    "### 8.3.4 좋은 스튜던트 선택하기\n",
    "\n",
    "### 8.3.5 옵투나로 좋은 하이퍼파라미터 찾기\n",
    "\n",
    "## 8.4 양자화로 모델 속도 높이기 \n",
    "\n",
    "## 8.5 양자화된 모델의 벤치마크 수행하기\n",
    "\n",
    "## 8.6 ONNX와 ONNX 런타임으로 추론 최적화하기\n",
    "\n",
    "## 8.7 가중치 가지치기로 희소한 모델 만들기\n",
    "\n",
    "### 8.7.1  심층 신경망의 희소성\n",
    "\n",
    "### 8.7.2 가중치 가지치기 방법\n",
    "\n",
    "## 8.8 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9f7ad-bb4f-4452-88db-0a6a07841e93",
   "metadata": {},
   "source": [
    "# Chapter 9. 레이블 부족 문제 다루기\n",
    "\n",
    "## 9.1 깃허브 이슈 태거 만들기\n",
    "\n",
    "### 9.1.1 데이터 다운로드하기\n",
    "\n",
    "### 9.1.2 데이터 준비하기\n",
    "\n",
    "### 9.1.3 훈련 세트 만들기\n",
    "\n",
    "### 9.1.4 훈련 슬라이스 만들기\n",
    "\n",
    "## 9.2 나이브 베이즈 모델 만들기\n",
    "\n",
    "## 9.3 레이블링된 데이터가 없는 경우\n",
    "\n",
    "## 9.4 레이블링된 데이터가 적은 경우\n",
    "\n",
    "### 9.4.1. 데이터 증식\n",
    "\n",
    "### 9.4.2 임베딩을 룩업 테이블로 사용하기\n",
    "\n",
    "### 9.4.3 기본 트랜스포머 미세 튜닝하기\n",
    "\n",
    "### 9.4.4 프롬프트를 사용한 인-컨텍스트 학습과 퓨샷 학습\n",
    "\n",
    "## 9.5 레이블링되지 않은 데이터 활용하기\n",
    "\n",
    "### 9.5.1 언어 모델 미세 튜닝하기\n",
    "\n",
    "### 9.5.2 분류기 미세 튜닝하기\n",
    "\n",
    "### 9.5.3 고급 방법\n",
    "\n",
    "## 9.6 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09467bae-9e8d-40c3-b434-18012a3beaf6",
   "metadata": {},
   "source": [
    "# Chapter 10. 대규모 데이터셋 수집하기\n",
    "\n",
    "## 10.1 대규모 데이터셋 수집하기\n",
    "\n",
    "### 10.1.1 대규모 말뭉치 구축의 어려움\n",
    "\n",
    "### 10.1.2 사용자 정의 코드 데이터셋 만들기\n",
    "\n",
    "### 10.1.3 대용량 데이터셋 다루기\n",
    "\n",
    "### 10.1.4 허깅페이스 허브에 데이터셋 추가하기\n",
    "\n",
    "## 10.2 토크나이저 구축하기\n",
    "\n",
    "### 10.2.1 토크나이저 모델\n",
    "\n",
    "### 10.2.2 토크나이저 성능 측정하기\n",
    "\n",
    "### 10.2.3 파이썬 코드를 위한 토크나이저\n",
    "\n",
    "### 10.2.4 토크나이저 훈련하기\n",
    "\n",
    "### 10.2.5 허브에 사용자 정의 토크나이저 저장하기\n",
    "\n",
    "## 10.3 밑바닥부터 모델을 훈련하기\n",
    "\n",
    "### 10.3.1 사전 훈련 목표\n",
    "\n",
    "### 10.3.2 모델 초기화\n",
    "\n",
    "### 10.3.3 데이터로 구축하기\n",
    "\n",
    "### 10.3.4 훈련 루프 정의하기\n",
    "\n",
    "### 10.3.5 훈련 실행\n",
    "\n",
    "## 10.4 결과 및 분석\n",
    "\n",
    "## 10.5 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca79b3-7f19-44e3-890a-16af778bd868",
   "metadata": {},
   "source": [
    "# Chapter 11. 향후 방향\n",
    "\n",
    "## 11.1 트랜스포머 확장\n",
    "\n",
    "### 11.1.1 규모의 법칙\n",
    "\n",
    "### 11.1.2 규모 확장의 어려움\n",
    "\n",
    "### 11.1.3 어텐션 플리즈!\n",
    "\n",
    "### 11.1.4 희소 어텐션\n",
    "\n",
    "### 11.1.5 선형 어텐션\n",
    "\n",
    "## 11.2 텍스트를 넘어서\n",
    "\n",
    "### 11.2.1 비전\n",
    "\n",
    "### 11.2.2 테이블 \n",
    "\n",
    "## 11.3 멀티모달 트랜스포머 \n",
    "\n",
    "### 11.3.1 스피치-투-텍스트\n",
    "\n",
    "### 11.3.2 비전과 텍스트\n",
    "\n",
    "## 11.4 다음 목적지는?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
