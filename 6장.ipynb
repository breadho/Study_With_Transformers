{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c94b13ed",
   "metadata": {},
   "source": [
    "# Chapter 6. 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f1029bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]\n",
      "3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772a0d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  5 23:05:06 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   40C    P5    23W / 170W |    347MiB / 12288MiB |     16%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      2196      G   /usr/lib/xorg/Xorg                168MiB |\r\n",
      "|    0   N/A  N/A      2337      G   /usr/bin/gnome-shell               35MiB |\r\n",
      "|    0   N/A  N/A      5066      G   ...870000447314327423,262144      120MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e71e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory:0.00 MB\n",
      "Cached Memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# GPU 메모리 사용량 확인\n",
    "if device.type ==\"cuda\":\n",
    "    allocated_memory = torch.cuda.memory_allocated(device = device)\n",
    "    cached_memory = torch.cuda.memory_reserved(device = device)\n",
    "\n",
    "print(f\"Allocated Memory:{allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Cached Memory: {cached_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a5d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab 수행 시 \n",
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be12f6",
   "metadata": {},
   "source": [
    "**(서론)**\n",
    "\n",
    "언젠가 한 번쯤 문서를 요약해야 할 때가 있습니다. 요약할 문서는 연구 논문이나 재무 실적 보고서 아니면 이메일 스레드일지도 모릅니다. 생각해보면 이런 작업에는 긴 단락을 이해하고, 관련 내용을 추론하고, 원래 문서의 주제를 통합해 유창한 텍스트를 생성하는 등 다양한 능력이 필요합니다. 또 기사를 정확하게 요약하는 방법과 법률 계약서를 요약하는 방법은 매우 다르기 때문에 정교한 수준의 도메인 일반화가 필요합니다. 이런 이유로 트랜스포머를 포함한 자연어 모델에게 텍스트 요약은 어려운 작업니다. 이런 어려움에도 불구하고 텍스트 요약은 도메인 전문가의 작업 속도를 크게 높이고 기업에서 내부 지식을 집약하고, 계약을 요약하고 소셜 미디어를 위한 자동 콘텐츠를 생성하는 등의 작업에 사용됩니다. \n",
    "\n",
    "\n",
    "이와 관련된 도전 과제를 이해하기 위해 이 장은 사전 훈련된 트랜스포머를 사용해 문서를 요약하는 방법을 알아보겠습니다. 요약은 입력과 출력이 텍스트인 고전적인 시퀀스-투-시퀀스 작업니다. 1장에서 보았듯이 요약에는 인코더-디코더 트랜스포머가 잘 맞습니다. \n",
    "\n",
    "\n",
    "이 장에서는 인코더-디코더 모델을 만들어 여러 사람이 주고 받은 대화를 간결하게 요약하겠습니다. \n",
    "\n",
    "하지만 그 전에 요약에 사용하는 대표적인 데이터 셋인 CNN/DAilyMail 말뭉치를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c3c5fd",
   "metadata": {},
   "source": [
    "## 6.1 CNN/DailyMail 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfba8f1",
   "metadata": {},
   "source": [
    "CNN/DailyMail 데이터셋은 300,000개 뉴스 기사와 요약의 쌍으로 구성됐습니다. 요약은 CNN과 DailyMail이 기사에 첨부한 글머리 목록의 내용인데, 요약이 본문에서 추출되지 않고 추상적이라는 중요한 특징이 있습니다. 즉, 단순한 발췌가 아니라 새로운 문장으로 구성됐다는 이야기입니다. 이 데이터 셋은 허깅 페이스 허브(https://huggingface.co/datasets/cnn_dailymail)에서 제공합니다. 여기서는 요약을 위해 익명화 처리를 하지 않은 3.0.0 버전을 사용하겠습니다. 4장에서 서브셋을 선택한 대와 비슷한 방식으로 version 매개변수를 사용해 버전을 선택합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3254d11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'COMMON_SAFE_ASCII_CHARACTERS' from 'charset_normalizer.constant' (/home/bread/anaconda3/envs/py31011/lib/python3.10/site-packages/charset_normalizer/constant.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/requests/compat.py:11\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chardet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn_dailymail\u001b[39m\u001b[38;5;124m\"\u001b[39m, version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m특성: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/datasets/__init__.py:43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pyarrow\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/datasets/arrow_dataset.py:60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi, HfFolder\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/huggingface_hub/__init__.py:305\u001b[0m, in \u001b[0;36m_attach.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m attr_to_modules:\n\u001b[1;32m    304\u001b[0m     submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     submod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(submod, name)\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/huggingface_hub/hf_api.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, BinaryIO, Dict, Iterable, Iterator, List, Optional, Tuple, Union\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quote\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     IGNORE_GIT_FOLDER_PATTERNS,\n\u001b[1;32m     32\u001b[0m     EntryNotFoundError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     get_session,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/requests/__init__.py:45\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m charset_normalizer_version\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/requests/exceptions.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mrequests.exceptions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mThis module contains the set of Requests' exceptions.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m BaseHTTPError\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m CompatJSONDecodeError\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRequestException\u001b[39;00m(\u001b[38;5;167;01mIOError\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"There was an ambiguous exception that occurred while handling your\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    request.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/requests/compat.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Pythons\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Syntax sugar.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/charset_normalizer/__init__.py:23\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, VERSION\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/charset_normalizer/api.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m     PathLike \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike[str]\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/charset_normalizer/md.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, List\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNICODE_SECONDARY_RANGE_KEYWORD\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_punctuation, is_symbol, unicode_range, is_accentuated, is_latin, \\\n\u001b[1;32m      6\u001b[0m     remove_accent, is_separator, is_cjk, is_case_variable, is_hangul, is_katakana, is_hiragana, is_ascii, is_thai\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMessDetectorPlugin\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Base abstract class used for mess detection plugins.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    All detectors MUST extend and implement given methods.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'COMMON_SAFE_ASCII_CHARACTERS' from 'charset_normalizer.constant' (/home/bread/anaconda3/envs/py31011/lib/python3.10/site-packages/charset_normalizer/constant.py)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", version = \"3.0.0\")\n",
    "print(f\"특성: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8f9683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory:0.00 MB\n",
      "Cached Memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# GPU 메모리 사용량 확인\n",
    "if device.type ==\"cuda\":\n",
    "    allocated_memory = torch.cuda.memory_allocated(device = device)\n",
    "    cached_memory = torch.cuda.memory_reserved(device = device)\n",
    "\n",
    "print(f\"Allocated Memory:{allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Cached Memory: {cached_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8856baad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result : 479.89 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# 현재 프로세스의 메모리 정보 얻기\n",
    "process = psutil.Process()\n",
    "print(f\"result : {process.memory_info().rss / 10**6:.2f} MB\")  # 현재 사용 중인 메모리 양 (Resident Set Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ec68b",
   "metadata": {},
   "source": [
    "이 데이터 셋은 세가지 특성이 있습니다. \n",
    "\n",
    " - 뉴스 기사를 담은 article\n",
    " \n",
    " - 요약에 해당하는 highlights\n",
    " \n",
    " - 기사의 고유 아이디 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740042bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 기사 중 하나의 내용을 발췌함\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m 기사 (500개 문자 발췌, 총 길이: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m500\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 기사 중 하나의 내용을 발췌함\n",
    "\n",
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\" 기사 (500개 문자 발췌, 총 길이: {len(sample[\"article\"])}):\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\n 요약 (길이: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa137909",
   "metadata": {},
   "source": [
    "기사가 요약에 비해 매우 긴 경우도 있습니다. 이 경우 17배나 차이가 납니다. 대부분 트랜스포머 모델의 문맥 크기가 몇 단락에 해당하는 분량인 1,000개 토큰 정도로 제한되므로, 긴 기사는 트랜스포머 모델에 문제를 일으깁니다. 이를 처리하는 표준적이면서 가장 단순한 방법은 모델의 문맥 크기에 맞춰 텍스트를 자른 것입니다. 텍스트 끝부분에 중요한 정보가 있다면 사라지겠지만, 이는 모델 구조의 제약으로 생기는 불가피한 선택입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff005db",
   "metadata": {},
   "source": [
    "## 6.2 텍스트 요약 파이프라인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0858654",
   "metadata": {},
   "source": [
    "앞의 예제 기사에 대한 출력을 정성적으로 살펴보면서 요약 작업에 많이 사용되는 트랜스포머 모델 몇 가지를 알아보겠습니다. \n",
    "\n",
    "살펴볼 모델 구조는 최대 입력 크기가 각각 다르지만 동일한 입력을 사용하고 출력을 비교하기 위해 입력 텍스트를 2,000자로 제한하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56bf364a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m2000\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 딕셔너리에 각 모델이 생성한 요약을 저장합니다.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m summaries \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "\n",
    "# 딕셔너리에 각 모델이 생성한 요약을 저장합니다.\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a754b",
   "metadata": {},
   "source": [
    "요약에서는 관례적으로 요약 문장을 줄바꿈으로 나눕니다. 마침표마다 그 뒤에 줄바꿈 토큰을 추가해도 되지만 그러면 'U.S'나 'U.N' 같은 문자열을 처리하지 못합니다. \n",
    "\n",
    "**NLTK(Natural Language Toolkit)** 패키지에는 문장의 종결과 약어에 등장하는 구두점을 구별하는 더 정교한 알고리즘이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6f78a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "122b59ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/nltk/__init__.py:138\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrammar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/nltk/text.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConditionalFreqDist \u001b[38;5;28;01mas\u001b[39;00m CFD\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyConcatenation, tokenwrap\n\u001b[1;32m     32\u001b[0m ConcordanceLine \u001b[38;5;241m=\u001b[39m namedtuple(\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcordanceLine\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_print\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright_print\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/nltk/tokenize/__init__.py:65\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcasual\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TweetTokenizer, casual_tokenize\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdestructive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NLTKWordTokenizer\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegality_principle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LegalitySyllableTokenizer\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/nltk/tokenize/casual.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhtml\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m  \u001b[38;5;66;03m# https://github.com/nltk/nltk/issues/2409\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenizerI\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# The following strings are components in the regular expression\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# that is used for tokenizing. It's important that phone_number\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# This particular element is used in a couple ways, so we define it\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# with a name:\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'regex'"
     ]
    }
   ],
   "source": [
    "import nltk as nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3751d612",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe U.S. are a country. The U.N. is an organization.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m sent_tokenize(string)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "string = \"The U.S. are a country. The U.N. is an organization.\"\n",
    "\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864be01",
   "metadata": {},
   "source": [
    "*에러 발생*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a60423",
   "metadata": {},
   "source": [
    "**WARNING** 다음절에서는 여러 개의 대규모 모델을 로드합니다. 메모리가 부족하다면 큰 모델을 작은 모델(가령 \"gpt\", \"t5-small\"로 바꾸거나 이 절을 건너뛰고 6.5절 'CNN/DailyMail' 데이터셋에서 pegasus 평가하기로 이동하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82ed49",
   "metadata": {},
   "source": [
    "### 6.2.1 요약 기준 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69df74",
   "metadata": {},
   "source": [
    "기사를 요약하는 일반적인 기준 모델(baseline)은 단순히 기사에서 맨 처음 문장 세개를 선택하는 것입니다. 이런 기준 모델은 NLTK 문장 토크나이저로 쉽게 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4874db48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthree_sentence_summary\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sent_tokenize(text)[:\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m summaries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m three_sentence_summary(\u001b[43msample_text\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_text' is not defined"
     ]
    }
   ],
   "source": [
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feed6a02",
   "metadata": {},
   "source": [
    "### 6.2.2 GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13339275",
   "metadata": {},
   "source": [
    "5장에서 GPT-2가 주어진 프롬프트로 텍스트를 생성하는 방법을 보았습니다. 이 모델은 입력 텍스트 뒤에 'TL;DR'을 추가해 요약을 생성하는 놀라운 기능을 발휘합니다. 너무 길어 읽지 않았다.(too long:didn't read)는 문구의 약어 'TL;DR'은 레딧(reddit) 같은 사이트에서 긴 포스트를 짧게 요약할 때 종종 사용됩니다. 허깅페이스 트랜스포머스의 pipeline() 함수로 원본 논문의 방식을 재현하며 요약 작업을 실험해 보겠습니다. 텍스트 생성 파이프라인을 만들고 대용량 GPT-2 모델을 로드합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88a68b27",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/requests/compat.py:11\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chardet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, set_seed\n\u001b[1;32m      3\u001b[0m set_seed(\u001b[38;5;241m42\u001b[39m) \n\u001b[1;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     logging,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     46\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/dependency_versions_check.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     26\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython tqdm regex requests packaging filelock numpy tokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/utils/__init__.py:30\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     24\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     ContextManagers,\n\u001b[1;32m     32\u001b[0m     ExplicitEnum,\n\u001b[1;32m     33\u001b[0m     ModelOutput,\n\u001b[1;32m     34\u001b[0m     PaddingStrategy,\n\u001b[1;32m     35\u001b[0m     TensorType,\n\u001b[1;32m     36\u001b[0m     cached_property,\n\u001b[1;32m     37\u001b[0m     can_return_loss,\n\u001b[1;32m     38\u001b[0m     expand_dims,\n\u001b[1;32m     39\u001b[0m     find_labels,\n\u001b[1;32m     40\u001b[0m     flatten_dict,\n\u001b[1;32m     41\u001b[0m     is_jax_tensor,\n\u001b[1;32m     42\u001b[0m     is_numpy_array,\n\u001b[1;32m     43\u001b[0m     is_tensor,\n\u001b[1;32m     44\u001b[0m     is_tf_tensor,\n\u001b[1;32m     45\u001b[0m     is_torch_device,\n\u001b[1;32m     46\u001b[0m     is_torch_dtype,\n\u001b[1;32m     47\u001b[0m     is_torch_tensor,\n\u001b[1;32m     48\u001b[0m     reshape,\n\u001b[1;32m     49\u001b[0m     squeeze,\n\u001b[1;32m     50\u001b[0m     strtobool,\n\u001b[1;32m     51\u001b[0m     tensor_size,\n\u001b[1;32m     52\u001b[0m     to_numpy,\n\u001b[1;32m     53\u001b[0m     to_py_obj,\n\u001b[1;32m     54\u001b[0m     transpose,\n\u001b[1;32m     55\u001b[0m     working_or_temp_dir,\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     58\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[1;32m     59\u001b[0m     DISABLE_TELEMETRY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     send_example_telemetry,\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     88\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[1;32m     89\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m     torch_version,\n\u001b[1;32m    168\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/utils/generic.py:29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, ContextManager, List, Tuple\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/utils/import_utils.py:32\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m importlib_metadata\n\u001b[1;32m     36\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/utils/logging.py:35\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     CRITICAL,  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     DEBUG,  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     WARNING,  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhf_hub_utils\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto \u001b[38;5;28;01mas\u001b[39;00m tqdm_lib\n\u001b[1;32m     39\u001b[0m _lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/huggingface_hub/utils/__init__.py:32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunk_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chunk_iterable\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_datetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_datetime\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     BadRequestError,\n\u001b[1;32m     34\u001b[0m     EntryNotFoundError,\n\u001b[1;32m     35\u001b[0m     GatedRepoError,\n\u001b[1;32m     36\u001b[0m     HfHubHTTPError,\n\u001b[1;32m     37\u001b[0m     LocalEntryNotFoundError,\n\u001b[1;32m     38\u001b[0m     RepositoryNotFoundError,\n\u001b[1;32m     39\u001b[0m     RevisionNotFoundError,\n\u001b[1;32m     40\u001b[0m     hf_raise_for_status,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SoftTemporaryDirectory, yaml_dump\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_git_credential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_credential_helpers, set_git_credential, unset_git_credential\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError, Response\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONDecodeError\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHfHubHTTPError\u001b[39;00m(HTTPError):\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/requests/__init__.py:45\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m charset_normalizer_version\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/requests/exceptions.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mrequests.exceptions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mThis module contains the set of Requests' exceptions.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m BaseHTTPError\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m CompatJSONDecodeError\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRequestException\u001b[39;00m(\u001b[38;5;167;01mIOError\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"There was an ambiguous exception that occurred while handling your\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    request.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/requests/compat.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Pythons\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Syntax sugar.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/charset_normalizer/__init__.py:23\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, VERSION\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/charset_normalizer/api.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m     PathLike \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike[str]\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42) \n",
    "pipe = pipeline(\"text-generation\", model = \"gpt2-xl\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_out = pipe(gpt2_query, max_length = 512, clean_up_tokenization_spaces = True)\n",
    "summaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691923f",
   "metadata": {},
   "source": [
    "나중의 비교를 위해 출력에서 입력 텍스트의 다음 부분을 요약으로 추출해 파이썬 딕셔너리에 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9fb845",
   "metadata": {},
   "source": [
    "### 6.2.3 T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e3699",
   "metadata": {},
   "source": [
    "다음에 시도할 모델은 T5 트랜스포머입니다. 3장에서 보았듯이, 이 모델의 개발자들은 NLP에서 포괄적인 전이 학습 연구를 수행해 모든 작업을 텍스트-투-텍스트 작업으로 구성하는 범용의 트랜스포머 아키텍처를 만들었습니다. \n",
    "\n",
    "T5 체크포인트는 요약을 포함해 여러 작업에서 (마슷킹된 단어를 재구성하기 위한) 비지도 학습 데이터와 지도 학습 데이터를 섞은 데이터로 훈련됐습니다. 따라서 미세 튜닝 없이 이 체크포인트를 사전 훈련에 썼던 것과 동일한 프롬프트를 사용해 바로 요약에 사용할 수 있습니다. \n",
    "\n",
    "문서 요약에 사용할 모델의 입력 포맷은 \"`summarize: <ARTICLE>`\" 이고, 번역에 사용할 입력 포맷은 \"`translate English to German: <TEXT>`\" 입니다. \n",
    "    \n",
    "[그림 6-1]에서 보듯이 이런 입력 포맷으로 T5는 많은 작업을 해결하는 매우 다재다능한 모델입니다. \n",
    "    \n",
    "요약을 위해 pipeline() 함수로 T5를 바로 로드하겠습니다. 이 함수는 입력을 text-to-text 포맷으로 처리하므로 앞에 \"summarize\"를 붙일 필요가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d5a10ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m pipe_out \u001b[38;5;241m=\u001b[39m pipe(sample_text)\n\u001b[1;32m      3\u001b[0m summaries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sent_tokenize(pipe_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model = \"t5-large\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fcea06",
   "metadata": {},
   "source": [
    "![그림6-1](image/chapter06_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32824b",
   "metadata": {},
   "source": [
    "### 6.2.4 BART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b98e1",
   "metadata": {},
   "source": [
    "BART도 인코더-디코더 구조를 사용하는 모델로, 손상된 입력을 재구성하도록 훈련됐습니다. 이를 위해 BERT와 GPT-2의 사전 훈련 방식을 결합합니다. 여기서는 특별히 CNN/DailyMail 데이터셋에 미세 튜닝된 facebook/bart-large-ccn 체크포인트를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e33a7ee6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m pipe_out \u001b[38;5;241m=\u001b[39m pipe(sample_text)\n\u001b[1;32m      3\u001b[0m summaries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbart\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sent_tokenize(pipe_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43b775",
   "metadata": {},
   "source": [
    "### 6.2.5 PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a80659",
   "metadata": {},
   "source": [
    "PEGASUS는 BART와 마찬가지로 인코더-디코더 트랜스포머입니다. [그림6-2]대로 이 모델은 여러 문장으로 구성된 텍스트에서 마스킹된 문장을 예측하는 사전 훈련 목표로 훈련되었습니다. 논문의 저자들은 사전 훈련 목표가 후속 작업에 가까울수록 더 효과적이라고 주장합니다. 일반적인 언어 모델링보다 요약에 특화된 사전 훈련 목표를 찾기 위해 대규모 말뭉치에서 (내용 중복을 측정하는 요약 평가 지표를 사용해) 주변 문단의 내용을 대부분 담은 문장을 자동으로 식별했습니다. 이런 문장을 재구성하도록 PEGASUS 모델을 사전 훈련해 최고 수준의 텍스트 요약 모델을 얻었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65917da",
   "metadata": {},
   "source": [
    "![그림6-2](image/chapter06_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd789e",
   "metadata": {},
   "source": [
    "이 모델은 줄 바꿈하는 특수 토큰이 있으므로 sent_tokenize() 함수를 사용할 필요가 없습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54fb003d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/pegasus-cnn-dailymail\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m pipe_out \u001b[38;5;241m=\u001b[39m pipe(sample_text)\n\u001b[1;32m      3\u001b[0m summaries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpegasus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pipe_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m .<n>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model = \"google/pegasus-cnn-dailymail\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab83d45",
   "metadata": {},
   "source": [
    "## 6.3 요약 결과 비교하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204ecd0",
   "metadata": {},
   "source": [
    "각기 다른 네 가지 모델로 요약을 생성했습니다. 결과를 비교해보죠. 한 모델(GPT-2)은 데이터셋에서 전혀 훈련되지 않았다는 점을 기억해 두세요. 한 모델 (T5)은 여러 작업 중의 하나로 이 작업을 위해 미세 튜닝되었습니다. 두 모델(BART와 PEGASUS)은 이 작업만을 위해 미세 튜닝되었습니다. \n",
    "\n",
    "네 모델이 생성한 요약 결과를 확인해 보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0570b5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUND TRUTH\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROUND TRUTH\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighlights\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m summaries:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"GROUND TRUTH\")\n",
    "print(dataset[\"train\"][1][\"highlights\"])\n",
    "print(\"\")\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58254f",
   "metadata": {},
   "source": [
    "--- 책 원문 내용 ---\n",
    "   \n",
    "모델 출력에서 가장 먼저 눈에 띄는 것은 GPT-2가 생성한 요약이 다른 결과와 크게 다른 것입니다. \n",
    "\n",
    "텍스트를 요약하는 대신 등장인물을 요약했습니다. \n",
    "\n",
    "GPT-2 모델은 진짜 요약을 생성하도록 명시적으로 훈련되지 않았기 때문에 종종 사실을 지어내거나 환상을 만들어 냅니다. \n",
    "\n",
    "예를 들어 이 글을 쓰는 시점에 Nesta의 순위는 1위가 아니라 9위 입니다. \n",
    "\n",
    "정답 요약과 다른 세 모델의 요약을 비교하면 놀랄 정도로 많이 중복되면 그 중 PEGASUS 출력과 가장 비슷합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedfd9d0",
   "metadata": {},
   "source": [
    "다음 절에서 생성된 텍스트의 품질을 측정하기 위해 개발된 일반적인 지표를 알아보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a038747",
   "metadata": {},
   "source": [
    "### 6.4 생성된 텍스트 품질 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e32995",
   "metadata": {},
   "source": [
    "평가 지표는 모델을 훈련할 때만이 아니라 나중에 제품 환경에서도 모델 성능을 평가하기 때문에 중요합니다. 평가 지표가 나쁘면 모델의 성능 저하를 눈치 채지 못하고, 평가 지표가 비즈니스 목표에 맞지 않으면 어떤 가치도 창출하지 못합니다. \n",
    "\n",
    "텍스트 생성 작업의 성능 측정은 감성 분석이나 개체명 인식 같은 표준적인 분류 작업만큼 쉽지 않습니다. \n",
    "\n",
    "생성된 텍스트를 평가하는 데 가장 널리 사용되는 두 지표는 BLEU와 ROUGE 입니다. 어떻게 정의되는지 알아보죠!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b32e15",
   "metadata": {},
   "source": [
    "### 6.4.1 BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f7b85",
   "metadata": {},
   "source": [
    "BLEU(BiLingual Evaluation Understudy)의 개념은 단순합니다. \n",
    "\n",
    "  - 생성된 텍스트에서 얼마나 많은 토큰이 참조 테스트 토큰과 완벽하게 똑같이 정렬됐는지 확인하는 대신, 단어 또는 n-그램을 체크합니다. \n",
    "\n",
    "  - BLEU는 정밀도를 근간으로 하는 지표입니다.\n",
    "  \n",
    "  - 두 텍스트를 비교할 때 참조 텍스트에 있는 단어가 생성된 텍스트에 얼마나 자주 등장하는 지 카운트합니다. 그 후에 생성된 텍스트 길이로 나눕니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97802a",
   "metadata": {},
   "source": [
    "하지만 이런 단순한 정밀도에는 문제가 있습니다. 생성된 텍스트에 동일 단어가 반복되고 이 단어가 참조 텍스트에 등장한다고 해보죠. 참조 텍스트 길이만큼 반복된다면 정밀도는 완벽합니다. 이런 이류도 BLEU 논문 저자들은 약간의 변화를 주었습니다. 단어를 참조 텍스트에 등장한 횟수만큼만 카운트합니다.\n",
    "\n",
    "이를 설명하기 위해 참조 텍스트가 'the cat is on the mat'이고 생성된 텍스트가 'the the the the the the'라고 가정해보죠.\n",
    "\n",
    "이 예시에서 정밀도는 이렇게 계산됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047fddbd",
   "metadata": {},
   "source": [
    "$$ P_{vanilla} = \\frac{6}{6} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070cba35",
   "metadata": {},
   "source": [
    "$$ P_{mod} = \\frac{2}{6} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f74b0b",
   "metadata": {},
   "source": [
    "간단한 수정으로 훨씬 합리적인 값을 얻었습니다. 이제 이를 확장해 단어 하나만이 아니라 n-그램도 확인할 수 있습니다. \n",
    "\n",
    "생성된 텍스트가 snt와 참조 문장 snt'를 비교한다고 해보죠. 특정 n에 대해 가능한 모든 n-그램을 추출해 정밀도를 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92832bab",
   "metadata": {},
   "source": [
    "$$ p_n = \\frac{\\sum_{n-gram \\in snt'}Count_{clip}(n-gram)}{\\sum_{n-gram \\in snt}Count(n-gram)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95731654",
   "metadata": {},
   "source": [
    "**반복적인 생성에 보상을 주지 않도록 분자의 카운트는 클리핑합니다.**\n",
    "\n",
    " - 생성된 문장에서 n-그램의 등장 횟수를 카운트하는 것이 참조 문장에 나타난 횟수로 제한된다는 의미입니다. \n",
    "\n",
    " - 이 식에서 문장의 정의는 그다지 엄격하지 않습니다. 여러 문장에 걸쳐 생성된 텍스트가 있다면 이를 하나의 문장으로 다룹니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b5b207",
   "metadata": {},
   "source": [
    "일반적으로 테스트 세트는 평가할 샘플이 하나 이상 있으니 말뭉치 C에 있는 모든 샘플을 더하도록 이 식을 조금 확장할 필요가 있습니다.\n",
    "\n",
    "$$ p_n = \\frac{\\sum_{snt' \\in C}\\sum_{n-gram \\in snt'} Count_{clip}(n-gram)}{\\sum_{snt\\in C}\\sum_{n-gram \\in snt} Count(n-gram) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647862f",
   "metadata": {},
   "source": [
    "거의 다 왔습니다. 재현율을 고려하지 않기 때문에 짧지만 정밀하게 생성된 시퀀스가 긴 문장보다 유리합니다. 따라서 짧게 생성된 텍스트의 정밀도 점수가 더 좋습니다. 이를 보상하기 위해 BLEU 논문의 저자들은 **브레비티 패널치(Brevity penalty)** 라는 추가 항을 도입했습니다.\n",
    "\n",
    "$$ BR = min(1, e^{1-l_{ref} \\; / \\; l_{gen}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cff702",
   "metadata": {},
   "source": [
    "```\n",
    "LaTeX에서 띄어쓰기를 하는 방법은 4가지가 있다.\n",
    "\n",
    "\\, : 한칸 띄어쓰기.\n",
    "\\; : 두칸 띄어쓰기.\n",
    "\\quad : 네칸(= tab) 띄어쓰기.\n",
    "\\qquad : 여덟칸(= tab*2) 띄어쓰기.\n",
    "이는 Mathjax에서도 똑같이 활용할 수 있다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3223220",
   "metadata": {},
   "source": [
    "최솟값을 선택하므로 이 패널티는 절대 1을 넘지 않고, 생성된 텍스트의 길이 $l_{gen}$ 가 참조 텍스트 $l_{ref}$ 보다 더 작을 때 지수 항이 기하급수적으로 작아집니다. 이 시점에서 왜 재현율도 고려하는 F1-score 같은 기준을 사용하지 않는지 궁금할 겁니다. 그 이유는 번역 데이터셋에는 하나가 아니라 여러 개의 참조 문장이 있는 경우가 있기 때문입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f658269",
   "metadata": {},
   "source": [
    "재현율을 측정하면 전체 참조 문장에 있는 단어를 모두 사용하는 번역에 인센티브가 주어집니다. 따라서 번역의 정밀도가 높고 번역과 참조 문장의 길이가 비슷한지 확인하는 것이 좋습니다. \n",
    "\n",
    "마지막으로 모든 것을 합쳐서 BLEU 점수를 계산하는 공식을 만들겠습니다.\n",
    "\n",
    "$$ BLEU -N = BR \\times (\\prod_{n=1}^{N}P_n)^{1/N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501e0de",
   "metadata": {},
   "source": [
    "마지막 항은 1에서 N까지 n-그램에서 수정 정밀도의 기하 평균입니다. BLEU-4 점수가 실제로 많이 사용됩니다. 하지만 이 지표에는 많은 제약이 있습니다. 한 예로, 동의어를 고려하지 않습니다. 유도된 식의 많은 단계가 임시 방편이고 깨지기 쉽습니다. BLEU의 단점을 잘 설명한 레이첼 타트만(Rachael Tatman)의 블로그 포스트 'Evaluating Text Output in NLP:BLEU at Your Own Risk'(https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)를 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284f90a",
   "metadata": {},
   "source": [
    "텍스트 생성 분야에서는 계속해서 더 좋은 평가 지표를 찾고 있습니다. BLEU 같은 지표의 단점을 극복하는 방법을 찾는 연구가 활발히 진행 중입니다. BLEU 지표의 또 다른 약점은 토큰화된 텍스트를 기대한다는 점입니다. 만약 텍스트 토큰화를 정확히 같은 방법으로 하지 않으면 결과가 달라집니다. SacreBLEU는 토큰화 단계를 내재화해 이 문제를 해결합니다. 이 때문에 벤치마킹에서는 이 지표를 선호합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e034a0",
   "metadata": {},
   "source": [
    "지금까지 이론적인 면을 살펴봤지만, 우리는 생성된 텍스트에 대해서 실제점수를 계산해야 합니다. 이 로직을 모두 파이썬으로 구현해야 할까요? 걱정하지마세요. 허깅페이스 데이터셋은 측정 지표도 제공합니다! 지표를 로딩하는 방법은 데이터셋을 로딩하는 방법과 비슷합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7e34e8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HfApi' from 'huggingface_hub' (/home/bread/anaconda3/envs/py31011/lib/python3.10/site-packages/huggingface_hub/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_metric\n\u001b[1;32m      3\u001b[0m bleu_metric \u001b[38;5;241m=\u001b[39m load_metrci(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacrebleu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/datasets/__init__.py:43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pyarrow\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/datasets/arrow_dataset.py:60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi, HfFolder\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HfApi' from 'huggingface_hub' (/home/bread/anaconda3/envs/py31011/lib/python3.10/site-packages/huggingface_hub/__init__.py)"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "bleu_metric = load_metrci(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2fc4b8",
   "metadata": {},
   "source": [
    "bleu_metric 객체는 Metric 클래스의 인스턴스로 하나의 수집기(aggregator)처럼 작동합니다. add() 메서드에 샘플 하나를 추가하거나 add_batch() 메서드로 배치 전체를 추가합니다. 평가하려는 샘플을 모두 추가한 다음 compute() 메서드를 호출하면 지표가 계산됩니다. 이 메서드는 몇 개의 값으로 구성된 딕셔너리를 반환합니다. 각 n-그램에 대한 정밀도, 길이 페널티, 최종 BLEU 점수 등입니다. 앞에서 예로 든 문장을 사용해보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8bab3c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bleu_metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m----> 4\u001b[0m \u001b[43mbleu_metric\u001b[49m\u001b[38;5;241m.\u001b[39madd(prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe the the the the the\u001b[39m\u001b[38;5;124m\"\u001b[39m, reference \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe cat is on the mat\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m bleu_metric\u001b[38;5;241m.\u001b[39mcompute(smooth_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloor\u001b[39m\u001b[38;5;124m\"\u001b[39m, smooth_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecisions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mround(p, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecisions\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bleu_metric' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "bleu_metric.add(prediction = \"the the the the the the\", reference = [\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method = \"floor\", smooth_value = 0)\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
    "pd.DataFrame.from_dict(results, orient = \"index\", columns = [\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7e97f",
   "metadata": {},
   "source": [
    "**NOTE** BLEU 점수는 여러 참조 번역이 있는 경우에도 계산됩니다. 이 때문에 reference 매개변수에 리스트를 전달합니다. BLEU는 정밀도 계산을 조금 바꿔 n-그램이 하나도 없을 때 최종 점수가 0이 되는 경우를 방지합니다. 이를 위한 방법으로 분자에 상수 값을 추가합니다. 이렇게 하면 n-그램이 없어도 점수가 0이 되지 않습니다. 이를 위한 방법으로 분자에 상수 값을 추가합니다. 이렇게 하면 n-그램이 없어도 점수가 0이 되지 않습니다. 이 값을 설명하기 위해 smooth_value = 0로 지정해 해당 기능을 껐습니다. \n",
    "\n",
    "- smooth_method가 floor이면 smooth_value의 기본값이 0.1이고, n-그램이 없을 경우 분자로 0.1을 사용합니다. smooth_method가 add-k이면 smooth_value의 기본값이 1이고, n-그램이 없을 경우 분모와 분자에 1이 더해집니다. smooth_method의 기본값은 'exp'이며 smooth_value를 사용하지 않고, n-그램이 없을 때마다 2의 거듭제곱을 누적해 분모에 곱한 역수를 계산합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2ee14",
   "metadata": {},
   "source": [
    "1- 그램의 정밀도는 실제로 2/6 입니다. 반면 2/3/4-그램의 정밀도는 모두 0입니다. counts와 bp 같은 개별 지표의 자세한 내용은 SacreBLEU 저장소(https://github.com/mjpost/sacrebleu)를 참고하세요. 그러면 기하 평균이 0이되므로 BLEU 점수도 0이 됩니다. 정밀도가 매우 높은 또 다른 예시를 확인해보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d04cfcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bleu_metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbleu_metric\u001b[49m\u001b[38;5;241m.\u001b[39madd(prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe cat is on mat\u001b[39m\u001b[38;5;124m\"\u001b[39m, reference \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe cat is on the mat\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m bleu_metric\u001b[38;5;241m.\u001b[39mcompute(smooth_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloor\u001b[39m\u001b[38;5;124m\"\u001b[39m, smooth_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecisions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mround(p, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecisions\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bleu_metric' is not defined"
     ]
    }
   ],
   "source": [
    "bleu_metric.add(prediction = \"the cat is on mat\", reference = [\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method = \"floor\", smooth_value = 0)\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
    "pd.DataFrame.from_dict(results, orient = \"index\", columns = [\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347ef36",
   "metadata": {},
   "source": [
    "BLEU 점수는 텍스트 평가에 널리 사용됩니다. 가능하고 적합한 단어를 모두 포함하는 번역보다 정확한 번역이 선호되기 때문에 특히 기계 번역에 많이 쓰입니다. \n",
    "\n",
    "이와 상황이 다른 요약 같은 애플리케이션이 있습니다. 이때는 중요한 정보가 생성된 텍스트에 모두 포함되어야 하므로 높은 재현율이 선호됩니다. 이런 작업에는 주로 ROUGE(Recall-Oriented Understudy for Gisting Evaluation)가 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf7ea5",
   "metadata": {},
   "source": [
    "### 6.4.2 ROUGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad094c3e",
   "metadata": {},
   "source": [
    "ROUGE 점수는 높은 재현율이 정밀도보다 훨씬 더 중요한 요약 같은 애플리케이션을 위해 특별히 개발됐습니다. 이 점수는 생성된 텍스트와 참조 텍스트에서 여러가지 n-그램이 얼마나 자주 등장하는지 비교한다는 점에서 BLEU와 매우 비슷합니다. 하지만 ROUGE는 참조 텍스트에 있는 n-그램이 생성된 텍스트에 얼마나 많이 등장하는지도 확인하다는 점이 다릅니다. BLEU는 생성된 텍스트에 있는 n-그램이 참조 텍스트에 얼마나 많이 등장하는지 봅니다. 따라서 ROUGE는 분모에서 참조 텍스트의 n-그램이 생성된 텍스트에 얼마나 많이 등장하는지(클리핑하지 않고) 카운트하도록 정밀도 공식을 조금 수정해 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691af970",
   "metadata": {},
   "source": [
    "$$ ROUGE-N = \\frac{\\sum_{snt' \\in C}\\sum_{n-gram \\in snt'} Count_{match}(n-gram)}{\\sum_{snt' \\in C}\\sum_{n-gram \\in snt'} Count(n-gram) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89ff86",
   "metadata": {},
   "source": [
    "이것이 ROUGE의 원래 공식입니다.\n",
    "\n",
    "연구자들은 정밀도를 완전히 제거하면 부정적인 영향이 커짐을 알았습니다. \n",
    "\n",
    "클리핑 카운트를 하지 않는 BLEU 공식으로 돌아가 정밀도를 측정한 다음 정밀도와 재현율 ROUGE 점수를 조화 평균하면 F1 점수가 나옵니다. \n",
    "\n",
    "이 점수가 오늘날 일반적으로 사용되는 ROUGE 점수입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba3e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43b8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06d213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294d499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30df38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d15ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf80e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba7c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac763cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef8bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85377401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348b159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
