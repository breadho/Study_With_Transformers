{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c94b13ed",
   "metadata": {},
   "source": [
    "# Chapter 6. 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f1029bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772a0d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul  2 15:40:03 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   39C    P5    23W / 170W |    504MiB / 12288MiB |     23%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      2214      G   /usr/lib/xorg/Xorg                228MiB |\r\n",
      "|    0   N/A  N/A      2351      G   /usr/bin/gnome-shell               93MiB |\r\n",
      "|    0   N/A  N/A      4590      G   ...218650441894694154,262144      159MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e71e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory:0.00 MB\n",
      "Cached Memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# GPU 메모리 사용량 확인\n",
    "if device.type ==\"cuda\":\n",
    "    allocated_memory = torch.cuda.memory_allocated(device = device)\n",
    "    cached_memory = torch.cuda.memory_reserved(device = device)\n",
    "\n",
    "print(f\"Allocated Memory:{allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Cached Memory: {cached_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4a5d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab 수행 시 \n",
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be12f6",
   "metadata": {},
   "source": [
    "**(서론)**\n",
    "\n",
    "언젠가 한 번쯤 문서를 요약해야 할 때가 있습니다. 요약할 문서는 연구 논문이나 재무 실적 보고서 아니면 이메일 스레드일지도 모릅니다. 생각해보면 이런 작업에는 긴 단락을 이해하고, 관련 내용을 추론하고, 원래 문서의 주제를 통합해 유창한 텍스트를 생성하는 등 다양한 능력이 필요합니다. 또 기사를 정확하게 요약하는 방법과 법률 계약서를 요약하는 방법은 매우 다르기 때문에 정교한 수준의 도메인 일반화가 필요합니다. 이런 이유로 트랜스포머를 포함한 자연어 모델에게 텍스트 요약은 어려운 작업니다. 이런 어려움에도 불구하고 텍스트 요약은 도메인 전문가의 작업 속도를 크게 높이고 기업에서 내부 지식을 집약하고, 계약을 요약하고 소셜 미디어를 위한 자동 콘텐츠를 생성하는 등의 작업에 사용됩니다. \n",
    "\n",
    "\n",
    "이와 관련된 도전 과제를 이해하기 위해 이 장은 사전 훈련된 트랜스포머를 사용해 문서를 요약하는 방법을 알아보겠습니다. 요약은 입력과 출력이 텍스트인 고전적인 시퀀스-투-시퀀스 작업니다. 1장에서 보았듯이 요약에는 인코더-디코더 트랜스포머가 잘 맞습니다. \n",
    "\n",
    "\n",
    "이 장에서는 인코더-디코더 모델을 만들어 여러 사람이 주고 받은 대화를 간결하게 요약하겠습니다. \n",
    "\n",
    "하지만 그 전에 요약에 사용하는 대표적인 데이터 셋인 CNN/DAilyMail 말뭉치를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c3c5fd",
   "metadata": {},
   "source": [
    "## 6.1 CNN/DailyMail 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfba8f1",
   "metadata": {},
   "source": [
    "CNN/DailyMail 데이터셋은 300,000개 뉴스 기사와 요약의 쌍으로 구성됐습니다. 요약은 CNN과 DailyMail이 기사에 첨부한 글머리 목록의 내용인데, 요약이 본문에서 추출되지 않고 추상적이라는 중요한 특징이 있습니다. 즉, 단순한 발췌가 아니라 새로운 문장으로 구성됐다는 이야기입니다. 이 데이터 셋은 허깅 페이스 허브(https://huggingface.co/datasets/cnn_dailymail)에서 제공합니다. 여기서는 요약을 위해 익명화 처리를 하지 않은 3.0.0 버전을 사용하겠습니다. 4장에서 서브셋을 선택한 대와 비슷한 방식으로 version 매개변수를 사용해 버전을 선택합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3254d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/bread/.cache/huggingface/datasets/cnn_dailymail/default/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd07c0ed8017491095c114c0fdb067fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성: ['article', 'highlights', 'id']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", version = \"3.0.0\")\n",
    "print(f\"특성: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8f9683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory:0.00 MB\n",
      "Cached Memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# GPU 메모리 사용량 확인\n",
    "if device.type ==\"cuda\":\n",
    "    allocated_memory = torch.cuda.memory_allocated(device = device)\n",
    "    cached_memory = torch.cuda.memory_reserved(device = device)\n",
    "\n",
    "print(f\"Allocated Memory:{allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Cached Memory: {cached_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8856baad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result : 464.91 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# 현재 프로세스의 메모리 정보 얻기\n",
    "process = psutil.Process()\n",
    "print(f\"result : {process.memory_info().rss / 10**6:.2f} MB\")  # 현재 사용 중인 메모리 양 (Resident Set Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ec68b",
   "metadata": {},
   "source": [
    "이 데이터 셋은 세가지 특성이 있습니다. \n",
    "\n",
    " - 뉴스 기사를 담은 article\n",
    " \n",
    " - 요약에 해당하는 highlights\n",
    " \n",
    " - 기사의 고유 아이디 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740042bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 기사 (500개 문자 발췌, 총 길이: 4051):\n",
      "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most s\n",
      "\n",
      " 요약 (길이: 281):\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n"
     ]
    }
   ],
   "source": [
    "# 기사 중 하나의 내용을 발췌함\n",
    "\n",
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\" 기사 (500개 문자 발췌, 총 길이: {len(sample[\"article\"])}):\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\n 요약 (길이: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa137909",
   "metadata": {},
   "source": [
    "기사가 요약에 비해 매우 긴 경우도 있습니다. 이 경우 17배나 차이가 납니다. 대부분 트랜스포머 모델의 문맥 크기가 몇 단락에 해당하는 분량인 1,000개 토큰 정도로 제한되므로, 긴 기사는 트랜스포머 모델에 문제를 일으깁니다. 이를 처리하는 표준적이면서 가장 단순한 방법은 모델의 문맥 크기에 맞춰 텍스트를 자른 것입니다. 텍스트 끝부분에 중요한 정보가 있다면 사라지겠지만, 이는 모델 구조의 제약으로 생기는 불가피한 선택입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff005db",
   "metadata": {},
   "source": [
    "## 6.2 텍스트 요약 파이프라인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0858654",
   "metadata": {},
   "source": [
    "앞의 예제 기사에 대한 출력을 정성적으로 살펴보면서 요약 작업에 많이 사용되는 트랜스포머 모델 몇 가지를 알아보겠습니다. \n",
    "\n",
    "살펴볼 모델 구조는 최대 입력 크기가 각각 다르지만 동일한 입력을 사용하고 출력을 비교하기 위해 입력 텍스트를 2,000자로 제한하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56bf364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "\n",
    "# 딕셔너리에 각 모델이 생성한 요약을 저장합니다.\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a754b",
   "metadata": {},
   "source": [
    "요약에서는 관례적으로 요약 문장을 줄바꿈으로 나눕니다. 마침표마다 그 뒤에 줄바꿈 토큰을 추가해도 되지만 그러면 'U.S'나 'U.N' 같은 문자열을 처리하지 못합니다. \n",
    "\n",
    "**NLTK(Natural Language Toolkit)** 패키지에는 문장의 종결과 약어에 등장하는 구두점을 구별하는 더 정교한 알고리즘이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6f78a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "122b59ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute '_no_nep50_warning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/nltk/__init__.py:133\u001b[0m\n\u001b[1;32m    125\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mPopen \u001b[38;5;241m=\u001b[39m _fake_Popen\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollocations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator, memoize\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/nltk/collocations.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_itertools\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     38\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     39\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     40\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspearman\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/nltk/metrics/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magreement\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnnotationTask\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m align\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massociation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     BigramAssocMeasures,\n\u001b[1;32m     20\u001b[0m     ContingencyMeasures,\n\u001b[1;32m     21\u001b[0m     NgramAssocMeasures,\n\u001b[1;32m     22\u001b[0m     QuadgramAssocMeasures,\n\u001b[1;32m     23\u001b[0m     TrigramAssocMeasures,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfusionmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     binary_distance,\n\u001b[1;32m     28\u001b[0m     custom_distance,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     presence,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/nltk/metrics/association.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m _SMALL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-20\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fisher_exact\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfisher_exact\u001b[39m(\u001b[38;5;241m*\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs):\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/scipy/stats/__init__.py:485\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 485\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/scipy/stats/_stats_py.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _measurements\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/numpy/testing/__init__.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munittest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestCase\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _private\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extbuild\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/numpy/testing/_private/utils.py:417\u001b[0m\n\u001b[1;32m    413\u001b[0m         pprint\u001b[38;5;241m.\u001b[39mpprint(desired, msg)\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;129m@np\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_nep50_warning\u001b[49m()\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_almost_equal\u001b[39m(actual, desired, decimal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, err_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    419\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    Raises an AssertionError if two items are not equal up to desired\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    precision.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m     __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Hide traceback for py.test\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/numpy/__init__.py:311\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    304\u001b[0m if attr in __former_attrs__:\n\u001b[1;32m    305\u001b[0m     raise AttributeError(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m # Importing Tester requires importing all of UnitTest which is not a\n\u001b[1;32m    308\u001b[0m # cheap import Since it is mainly used in test suits, we lazy import it\n\u001b[1;32m    309\u001b[0m # here to save on the order of 10 ms of import time for most users\n\u001b[1;32m    310\u001b[0m #\n\u001b[0;32m--> 311\u001b[0m # The previous way Tester was imported also had a side effect of adding\n\u001b[1;32m    312\u001b[0m # the full `numpy.testing` namespace\n\u001b[1;32m    313\u001b[0m if attr == 'testing':\n\u001b[1;32m    314\u001b[0m     import numpy.testing as testing\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute '_no_nep50_warning'"
     ]
    }
   ],
   "source": [
    "import nltk as nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "string = \"The U.S. are a country. The U.N. is an organization.\"\n",
    "\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864be01",
   "metadata": {},
   "source": [
    "*에러 발생*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a60423",
   "metadata": {},
   "source": [
    "**WARNING** 다음절에서는 여러 개의 대규모 모델을 로드합니다. 메모리가 부족하다면 큰 모델을 작은 모델(가령 \"gpt\", \"t5-small\"로 바꾸거나 이 절을 건너뛰고 6.5절 'CNN/DailyMail' 데이터셋에서 pegasus 평가하기로 이동하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82ed49",
   "metadata": {},
   "source": [
    "### 6.2.1 요약 기준 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69df74",
   "metadata": {},
   "source": [
    "기사를 요약하는 일반적인 기준 모델(baseline)은 단순히 기사에서 맨 처음 문장 세개를 선택하는 것입니다. 이런 기준 모델은 NLTK 문장 토크나이저로 쉽게 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4874db48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthree_sentence_summary\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sent_tokenize(text)[:\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m summaries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mthree_sentence_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m, in \u001b[0;36mthree_sentence_summary\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthree_sentence_summary\u001b[39m(text):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43msent_tokenize\u001b[49m(text)[:\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feed6a02",
   "metadata": {},
   "source": [
    "### 6.2.2 GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13339275",
   "metadata": {},
   "source": [
    "5장에서 GPT-2가 주어진 프롬프트로 텍스트를 생성하는 방법을 보았습니다. 이 모델은 입력 텍스트 뒤에 'TL;DR'을 추가해 요약을 생성하는 놀라운 기능을 발휘합니다. 너무 길어 읽지 않았다.(too long:didn't read)는 문구의 약어 'TL;DR'은 레딧(reddit) 같은 사이트에서 긴 포스트를 짧게 요약할 때 종종 사용됩니다. 허깅페이스 트랜스포머스의 pipeline() 함수로 원본 논문의 방식을 재현하며 요약 작업을 실험해 보겠습니다. 텍스트 생성 파이프라인을 만들고 대용량 GPT-2 모델을 로드합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88a68b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 16:41:52.482451: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-02 16:41:52.692703: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-02 16:41:53.224118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute '_no_nep50_warning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/utils/import_utils.py:1146\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/pipelines/__init__.py:60\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdepth_estimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DepthEstimationPipeline\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_question_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentQuestionAnsweringPipeline\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureExtractionPipeline\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/pipelines/document_question_answering.py:29\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PIPELINE_INIT_ARGS, ChunkPipeline\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquestion_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_starts_ends\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/pipelines/question_answering.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SquadExample, SquadFeatures, squad_convert_examples_to_features\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCard\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/data/__init__.py:26\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[1;32m     17\u001b[0m     DataCollatorForPermutationLanguageModeling,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     default_data_collator,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glue_compute_metrics, xnli_compute_metrics\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     DataProcessor,\n\u001b[1;32m     29\u001b[0m     InputExample,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     xnli_tasks_num_labels,\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_sklearn_available, requires_backends\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_sklearn_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pearsonr, spearmanr\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/utils/import_utils.py:565\u001b[0m, in \u001b[0;36mis_sklearn_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_scipy_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msklearn.metrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/importlib/util.py:94\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parent_name:\n\u001b[0;32m---> 94\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfromlist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__path__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/sklearn/__init__.py:82\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __check_build  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/sklearn/base.py:17\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_set_output\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _SetOutputMixin\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/sklearn/utils/__init__.py:25\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m all_estimators\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version, threadpool_info\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/sklearn/utils/fixes.py:19\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreadpoolctl\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/scipy/stats/__init__.py:485\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 485\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/scipy/stats/_stats_py.py:37\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/numpy/testing/__init__.py:11\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _private\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/numpy/testing/_private/utils.py:417\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;129m@np\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_nep50_warning\u001b[49m()\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_almost_equal\u001b[39m(actual, desired, decimal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, err_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    419\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    Raises an AssertionError if two items are not equal up to desired\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    precision.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/numpy/__init__.py:311\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    305\u001b[0m     raise AttributeError(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m # Importing Tester requires importing all of UnitTest which is not a\n\u001b[1;32m    308\u001b[0m # cheap import Since it is mainly used in test suits, we lazy import it\n\u001b[1;32m    309\u001b[0m # here to save on the order of 10 ms of import time for most users\n\u001b[1;32m    310\u001b[0m #\n\u001b[0;32m--> 311\u001b[0m # The previous way Tester was imported also had a side effect of adding\n\u001b[1;32m    312\u001b[0m # the full `numpy.testing` namespace\n\u001b[1;32m    313\u001b[0m if attr == 'testing':\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute '_no_nep50_warning'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, set_seed\n\u001b[1;32m      3\u001b[0m set_seed(\u001b[38;5;241m42\u001b[39m) \n\u001b[1;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/utils/import_utils.py:1136\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1134\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1136\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py31011/lib/python3.10/site-packages/transformers/utils/import_utils.py:1148\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1151\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute '_no_nep50_warning'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42) \n",
    "pipe = pipeline(\"text-generation\", model = \"gpt2-xl\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_out = pipe(gpt2_query, max_length = 512, clean_up_tokenization_spaces = True)\n",
    "summaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691923f",
   "metadata": {},
   "source": [
    "나중의 비교를 위해 출력에서 입력 텍스트의 다음 부분을 요약으로 추출해 파이썬 딕셔너리에 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9fb845",
   "metadata": {},
   "source": [
    "### 6.2.3 T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e3699",
   "metadata": {},
   "source": [
    "다음에 시도할 모델은 T5 트랜스포머입니다. 3장에서 보았듯이, 이 모델의 개발자들은 NLP에서 포괄적인 전이 학습 연구를 수행해 모든 작업을 텍스트-투-텍스트 작업으로 구성하는 범용의 트랜스포머 아키텍처를 만들었습니다. \n",
    "\n",
    "T5 체크포인트는 요약을 포함해 여러 작업에서 (마슷킹된 단어를 재구성하기 위한) 비지도 학습 데이터와 지도 학습 데이터를 섞은 데이터로 훈련됐습니다. 따라서 미세 튜닝 없이 이 체크포인트를 사전 훈련에 썼던 것과 동일한 프롬프트를 사용해 바로 요약에 사용할 수 있습니다. \n",
    "\n",
    "문서 요약에 사용할 모델의 입력 포맷은 \"`summarize: <ARTICLE>`\" 이고, 번역에 사용할 입력 포맷은 \"`translate English to German: <TEXT>`\" 입니다. \n",
    "    \n",
    "[그림 6-1]에서 보듯이 이런 입력 포맷으로 T5는 많은 작업을 해결하는 매우 다재다능한 모델입니다. \n",
    "    \n",
    "요약을 위해 pipeline() 함수로 T5를 바로 로드하겠습니다. 이 함수는 입력을 text-to-text 포맷으로 처리하므로 앞에 \"summarize\"를 붙일 필요가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d5a10ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m pipe_out \u001b[38;5;241m=\u001b[39m pipe(sample_text)\n\u001b[1;32m      3\u001b[0m summaries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sent_tokenize(pipe_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model = \"t5-large\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fcea06",
   "metadata": {},
   "source": [
    "![그림6-1](image/chapter06_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32824b",
   "metadata": {},
   "source": [
    "### 6.2.4 BART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b98e1",
   "metadata": {},
   "source": [
    "BART도 인코더-디코더 구조를 사용하는 모델로, 손상된 입력을 재구성하도록 훈련됐습니다. 이를 위해 BERT와 GPT-2의 사전 훈련 방식을 결합합니다. 여기서는 특별히 CNN/DailyMail 데이터셋에 미세 튜닝된 facebook/bart-large-ccn 체크포인트를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e33a7ee6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m pipe_out \u001b[38;5;241m=\u001b[39m pipe(sample_text)\n\u001b[1;32m      3\u001b[0m summaries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbart\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sent_tokenize(pipe_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43b775",
   "metadata": {},
   "source": [
    "### 6.2.5 PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a80659",
   "metadata": {},
   "source": [
    "PEGASUS는 BART와 마찬가지로 인코더-디코더 트랜스포머입니다. [그림6-2]대로 이 모델은 여러 문장으로 구성된 텍스트에서 마스킹된 문장을 예측하는 사전 훈련 목표로 훈련되었습니다. 논문의 저자들은 사전 훈련 목표가 후속 작업에 가까울수록 더 효과적이라고 주장합니다. 일반적인 언어 모델링보다 요약에 특화된 사전 훈련 목표를 찾기 위해 대규모 말뭉치에서 (내용 중복을 측정하는 요약 평가 지표를 사용해) 주변 문단의 내용을 대부분 담은 문장을 자동으로 식별했습니다. 이런 문장을 재구성하도록 PEGASUS 모델을 사전 훈련해 최고 수준의 텍스트 요약 모델을 얻었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65917da",
   "metadata": {},
   "source": [
    "![그림6-2](image/chapter06_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd789e",
   "metadata": {},
   "source": [
    "이 모델은 줄 바꿈하는 특수 토큰이 있으므로 sent_tokenize() 함수를 사용할 필요가 없습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54fb003d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/pegasus-cnn-dailymail\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m pipe_out \u001b[38;5;241m=\u001b[39m pipe(sample_text)\n\u001b[1;32m      3\u001b[0m summaries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpegasus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pipe_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m .<n>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model = \"google/pegasus-cnn-dailymail\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab83d45",
   "metadata": {},
   "source": [
    "## 6.3 요약 결과 비교하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204ecd0",
   "metadata": {},
   "source": [
    "각기 다른 네 가지 모델로 요약을 생성했습니다. 결과를 비교해보죠. 한 모델(GPT-2)은 데이터셋에서 전혀 훈련되지 않았다는 점을 기억해 두세요. 한 모델 (T5)은 여러 작업 중의 하나로 이 작업을 위해 미세 튜닝되었습니다. 두 모델(BART와 PEGASUS)은 이 작업만을 위해 미세 튜닝되었습니다. \n",
    "\n",
    "네 모델이 생성한 요약 결과를 확인해 보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0570b5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUND TRUTH\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GROUND TRUTH\")\n",
    "print(dataset[\"train\"][1][\"highlights\"])\n",
    "print(\"\")\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58254f",
   "metadata": {},
   "source": [
    "--- 책 원문 내용 ---\n",
    "   \n",
    "모델 출력에서 가장 먼저 눈에 띄는 것은 GPT-2가 생성한 요약이 다른 결과와 크게 다른 것입니다. \n",
    "\n",
    "텍스트를 요약하는 대신 등장인물을 요약했습니다. \n",
    "\n",
    "GPT-2 모델은 진짜 요약을 생성하도록 명시적으로 훈련되지 않았기 때문에 종종 사실을 지어내거나 환상을 만들어 냅니다. \n",
    "\n",
    "예를 들어 이 글을 쓰는 시점에 Nesta의 순위는 1위가 아니라 9위 입니다. \n",
    "\n",
    "정답 요약과 다른 세 모델의 요약을 비교하면 놀랄 정도로 많이 중복되면 그 중 PEGASUS 출력과 가장 비슷합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedfd9d0",
   "metadata": {},
   "source": [
    "다음 절에서 생성된 텍스트의 품질을 측정하기 위해 개발된 일반적인 지표를 알아보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a038747",
   "metadata": {},
   "source": [
    "### 6.4 생성된 텍스트 품질 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e32995",
   "metadata": {},
   "source": [
    "평가 지표는 모델을 훈련할 때만이 아니라 나중에 제품 환경에서도 모델 성능을 평가하기 때문에 중요합니다. 평가 지표가 나쁘면 모델의 성능 저하를 눈치 채지 못하고, 평가 지표가 비즈니스 목표에 맞지 않으면 어떤 가치도 창출하지 못합니다. \n",
    "\n",
    "텍스트 생성 작업의 성능 측정은 감성 분석이나 개체명 인식 같은 표준적인 분류 작업만큼 쉽지 않습니다. \n",
    "\n",
    "생성된 텍스트를 평가하는 데 가장 널리 사용되는 두 지표는 BLEU와 ROUGE 입니다. 어떻게 정의되는지 알아보죠!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b32e15",
   "metadata": {},
   "source": [
    "### 6.4.1 BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f7b85",
   "metadata": {},
   "source": [
    "BLEU(BiLingual Evaluation Understudy)의 개념은 단순합니다. \n",
    "\n",
    "  - 생성된 텍스트에서 얼마나 많은 토큰이 참조 테스트 토큰과 완벽하게 똑같이 정렬됐는지 확인하는 대신, 단어 또는 n-그램을 체크합니다. \n",
    "\n",
    "  - BLEU는 정밀도를 근간으로 하는 지표입니다.\n",
    "  \n",
    "  - 두 텍스트를 비교할 때 참조 텍스트에 있는 단어가 생성된 텍스트에 얼마나 자주 등장하는 지 카운트합니다. 그 후에 생성된 텍스트 길이로 나눕니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97802a",
   "metadata": {},
   "source": [
    "하지만 이런 단순한 정밀도에는 문제가 있습니다. 생성된 텍스트에 동일 단어가 반복되고 이 단어가 참조 텍스트에 등장한다고 해보죠. 참조 텍스트 길이만큼 반복된다면 정밀도는 완벽합니다. 이런 이류도 BLEU 논문 저자들은 약간의 변화를 주었습니다. 단어를 참조 텍스트에 등장한 횟수만큼만 카운트합니다.\n",
    "\n",
    "이를 설명하기 위해 참조 텍스트가 'the cat is on the mat'이고 생성된 텍스트가 'the the the the the the'라고 가정해보죠.\n",
    "\n",
    "이 예시에서 정밀도는 이렇게 계산됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047fddbd",
   "metadata": {},
   "source": [
    "$$ P_{vanilla} = \\frac{6}{6} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070cba35",
   "metadata": {},
   "source": [
    "$$ P_{mod} = \\frac{2}{6} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f74b0b",
   "metadata": {},
   "source": [
    "간단한 수정으로 훨씬 합리적인 값을 얻었습니다. 이제 이를 확장해 단어 하나만이 아니라 n-그램도 확인할 수 있습니다. \n",
    "\n",
    "생성된 텍스트가 snt와 참조 문장 snt'를 비교한다고 해보죠. 특정 n에 대해 가능한 모든 n-그램을 추출해 정밀도를 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92832bab",
   "metadata": {},
   "source": [
    "$$ p_n = \\frac{\\sum_{n-gram \\in snt'}Count_{clip}(n-gram)}{\\sum_{n-gram \\in snt}Count(n-gram)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95731654",
   "metadata": {},
   "source": [
    "**반복적인 생성에 보상을 주지 않도록 분자의 카운트는 클리핑합니다.**\n",
    "\n",
    " - 생성된 문장에서 n-그램의 등장 횟수를 카운트하는 것이 참조 문장에 나타난 횟수로 제한된다는 의미입니다. \n",
    "\n",
    " - 이 식에서 문장의 정의는 그다지 엄격하지 않습니다. 여러 문장에 걸쳐 생성된 텍스트가 있다면 이를 하나의 문장으로 다룹니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b5b207",
   "metadata": {},
   "source": [
    "일반적으로 테스트 세트는 평가할 샘플이 하나 이상 있으니 말뭉치 C에 있는 모든 샘플을 더하도록 이 식을 조금 확장할 필요가 있습니다.\n",
    "\n",
    "$$ p_n = \\frac{\\sum_{snt' \\in C}\\sum_{n-gram \\in snt'} Count_{clip}(n-gram)}{\\sum_{snt\\in C}\\sum_{n-gram \\in snt} Count(n-gram) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647862f",
   "metadata": {},
   "source": [
    "거의 다 왔습니다. 재현율을 고려하지 않기 때문에 짧지만 정밀하게 생성된 시퀀스가 긴 문장보다 유리합니다. 따라서 짧게 생성된 텍스트의 정밀도 점수가 더 좋습니다. 이를 보상하기 위해 BLEU 논문의 저자들은 **브레비티 패널치(Brevity penalty)** 라는 추가 항을 도입했습니다.\n",
    "\n",
    "$$ BR = min(1, e^{1-l_{ref} \\; / \\; l_{gen}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cff702",
   "metadata": {},
   "source": [
    "```\n",
    "LaTeX에서 띄어쓰기를 하는 방법은 4가지가 있다.\n",
    "\n",
    "\\, : 한칸 띄어쓰기.\n",
    "\\; : 두칸 띄어쓰기.\n",
    "\\quad : 네칸(= tab) 띄어쓰기.\n",
    "\\qquad : 여덟칸(= tab*2) 띄어쓰기.\n",
    "이는 Mathjax에서도 똑같이 활용할 수 있다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3223220",
   "metadata": {},
   "source": [
    "최솟값을 선택하므로 이 패널티는 절대 1을 넘지 않고, 생성된 텍스트의 길이 $l_{gen}$ 가 참조 텍스트 $l_{ref}$ 보다 더 작을 때 지수 항이 기하급수적으로 작아집니다. 이 시점에서 왜 재현율도 고려하는 F1-score 같은 기준을 사용하지 않는지 궁금할 겁니다. 그 이유는 번역 데이터셋에는 하나가 아니라 여러 개의 참조 문장이 있는 경우가 있기 때문입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f658269",
   "metadata": {},
   "source": [
    "재현율을 측정하면 전체 참조 문장에 있는 단어를 모두 사용하는 번역에 인센티브가 주어집니다. 따라서 번역의 정밀도가 높고 번역과 참조 문장의 길이가 비슷한지 확인하는 것이 좋습니다. \n",
    "\n",
    "마지막으로 모든 것을 합쳐서 BLEU 점수를 계산하는 공식을 만들겠습니다.\n",
    "\n",
    "$$ BLEU -N = BR \\times (\\prod_{n=1}^{N}P_n)^{1/N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501e0de",
   "metadata": {},
   "source": [
    "마지막 항은 1에서 N까지 n-그램에서 수정 정밀도의 기하 평균입니다. BLEU-4 점수가 실제로 많이 사용됩니다. 하지만 이 지표에는 많은 제약이 있습니다. 한 예로, 동의어를 고려하지 않습니다. 유도된 식의 많은 단계가 임시 방편이고 깨지기 쉽습니다. BLEU의 단점을 잘 설명한 레이첼 타트만(Rachael Tatman)의 블로그 포스트 'Evaluating Text Output in NLP:BLEU at Your Own Risk'(https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)를 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284f90a",
   "metadata": {},
   "source": [
    "텍스트 생성 분야에서는 계속해서 더 좋은 평가 지표를 찾고 있습니다. BLEU 같은 지표의 단점을 극복하는 방법을 찾는 연구가 활발히 진행 중입니다. BLEU 지표의 또 다른 약점은 토큰화된 텍스트를 기대한다는 점입니다. 만약 텍스트 토큰화를 정확히 같은 방법으로 하지 않으면 결과가 달라집니다. SacreBLEU는 토큰화 단계를 내재화해 이 문제를 해결합니다. 이 때문에 벤치마킹에서는 이 지표를 선호합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e034a0",
   "metadata": {},
   "source": [
    "지금까지 이론적인 면을 살펴봤지만, 우리는 생성된 텍스트에 대해서 실제점수를 계산해야 합니다. 이 로직을 모두 파이썬으로 구현해야 할까요? 걱정하지마세요. 허깅페이스 데이터셋은 측정 지표도 제공합니다! 지표를 로딩하는 방법은 데이터셋을 로딩하는 방법과 비슷합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7e34e8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_metrci' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_metric\n\u001b[0;32m----> 3\u001b[0m bleu_metric \u001b[38;5;241m=\u001b[39m \u001b[43mload_metrci\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacrebleu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_metrci' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "bleu_metric = load_metrci(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2fc4b8",
   "metadata": {},
   "source": [
    "bleu_metric 객체는 Metric 클래스의 인스턴스로 하나의 수집기(aggregator)처럼 작동합니다. add() 메서드에 샘플 하나를 추가하거나 add_batch() 메서드로 배치 전체를 추가합니다. 평가하려는 샘플을 모두 추가한 다음 compute() 메서드를 호출하면 지표가 계산됩니다. 이 메서드는 몇 개의 값으로 구성된 딕셔너리를 반환합니다. 각 n-그램에 대한 정밀도, 길이 페널티, 최종 BLEU 점수 등입니다. 앞에서 예로 든 문장을 사용해보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8bab3c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bleu_metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m----> 4\u001b[0m \u001b[43mbleu_metric\u001b[49m\u001b[38;5;241m.\u001b[39madd(prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe the the the the the\u001b[39m\u001b[38;5;124m\"\u001b[39m, reference \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe cat is on the mat\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m bleu_metric\u001b[38;5;241m.\u001b[39mcompute(smooth_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloor\u001b[39m\u001b[38;5;124m\"\u001b[39m, smooth_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecisions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mround(p, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecisions\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bleu_metric' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "bleu_metric.add(prediction = \"the the the the the the\", reference = [\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method = \"floor\", smooth_value = 0)\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
    "pd.DataFrame.from_dict(results, orient = \"index\", columns = [\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7e97f",
   "metadata": {},
   "source": [
    "**NOTE** BLEU 점수는 여러 참조 번역이 있는 경우에도 계산됩니다. 이 때문에 reference 매개변수에 리스트를 전달합니다. BLEU는 정밀도 계산을 조금 바꿔 n-그램이 하나도 없을 때 최종 점수가 0이 되는 경우를 방지합니다. 이를 위한 방법으로 분자에 상수 값을 추가합니다. 이렇게 하면 n-그램이 없어도 점수가 0이 되지 않습니다. 이를 위한 방법으로 분자에 상수 값을 추가합니다. 이렇게 하면 n-그램이 없어도 점수가 0이 되지 않습니다. 이 값을 설명하기 위해 smooth_value = 0로 지정해 해당 기능을 껐습니다. \n",
    "\n",
    "- smooth_method가 floor이면 smooth_value의 기본값이 0.1이고, n-그램이 없을 경우 분자로 0.1을 사용합니다. smooth_method가 add-k이면 smooth_value의 기본값이 1이고, n-그램이 없을 경우 분모와 분자에 1이 더해집니다. smooth_method의 기본값은 'exp'이며 smooth_value를 사용하지 않고, n-그램이 없을 때마다 2의 거듭제곱을 누적해 분모에 곱한 역수를 계산합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2ee14",
   "metadata": {},
   "source": [
    "1- 그램의 정밀도는 실제로 2/6 입니다. 반면 2/3/4-그램의 정밀도는 모두 0입니다. counts와 bp 같은 개별 지표의 자세한 내용은 SacreBLEU 저장소(https://github.com/mjpost/sacrebleu)를 참고하세요. 그러면 기하 평균이 0이되므로 BLEU 점수도 0이 됩니다. 정밀도가 매우 높은 또 다른 예시를 확인해보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d04cfcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bleu_metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbleu_metric\u001b[49m\u001b[38;5;241m.\u001b[39madd(prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe cat is on mat\u001b[39m\u001b[38;5;124m\"\u001b[39m, reference \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe cat is on the mat\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m bleu_metric\u001b[38;5;241m.\u001b[39mcompute(smooth_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloor\u001b[39m\u001b[38;5;124m\"\u001b[39m, smooth_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecisions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mround(p, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecisions\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bleu_metric' is not defined"
     ]
    }
   ],
   "source": [
    "bleu_metric.add(prediction = \"the cat is on mat\", reference = [\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method = \"floor\", smooth_value = 0)\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
    "pd.DataFrame.from_dict(results, orient = \"index\", columns = [\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347ef36",
   "metadata": {},
   "source": [
    "BLEU 점수는 텍스트 평가에 널리 사용됩니다. 가능하고 적합한 단어를 모두 포함하는 번역보다 정확한 번역이 선호되기 때문에 특히 기계 번역에 많이 쓰입니다. \n",
    "\n",
    "이와 상황이 다른 요약 같은 애플리케이션이 있습니다. 이때는 중요한 정보가 생성된 텍스트에 모두 포함되어야 하므로 높은 재현율이 선호됩니다. 이런 작업에는 주로 ROUGE(Recall-Oriented Understudy for Gisting Evaluation)가 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055a313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660efbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ad6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8decd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba3e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43b8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06d213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294d499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30df38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d15ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf80e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba7c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac763cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef8bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85377401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348b159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
