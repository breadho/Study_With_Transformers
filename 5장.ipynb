{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184de511",
   "metadata": {},
   "source": [
    "# Chapter 5. 텍스트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750c615",
   "metadata": {},
   "source": [
    "- 트랜스포머 기반 언어 모델은 사람이 작성한 텍스트와 거의 구분이 되지 않는 텍스트를 생성하는 매우 신기한 능력을 발휘\n",
    "\n",
    "- 어떤 명시적인 감독(supervision)도 없이 텍스트를 생성했다는 점에서 매우 놀라움\n",
    "\n",
    "- GPT-2와 더 강력한 후속 모델인 GPT-3는 수백만 개의 웹 페이지에서 단순히 다음 단어를 예측하는 방법을 학습해서, 다양한 종류의 입력 프롬프트를 바탕으로 글을 생성할 수 있는 광범위한 기술과 패턴 인식을 습득\n",
    "\n",
    "- 언어 모델이 사전 훈련하는 동안 덧셈, 단어 철자 배열, 번역 같은 문맥 기반으로 다음 토큰을 예측하는 작업 시퀀스에 어떻게 노출되는지 보여줌\n",
    "\n",
    "  - 이를 통해 얻은 지깃은 미세튜닝이나 (모델이 충분히 크다면) 추론 과정에서 효율적으로 전이(transfer)됩니다.\n",
    "  \n",
    "  - 이런 작업은 사전에 선택된 것이 아니며 파라미터가 수십억 개인 언어 모델을 훈련하는 대규모 말뭉치에 자연스럽게 등장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e724748",
   "metadata": {},
   "source": [
    "![그림5-1](image/chapter05_lm-meta-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5f22c",
   "metadata": {},
   "source": [
    "* 현재는 Chat GPT(GPT-3.5, GPT-4 모델로 세계를 광풍에 몰아 넣고 있음, 23년도 5월 기준)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f806d",
   "metadata": {},
   "source": [
    "이 장에서는 GPT-2를 사용해 언어 모델의 텍스트 생성 원리를 설명하고, 다양한 디코딩 전략이 생성된 텍스트에 미치는 영향을 살펴봄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e89e8",
   "metadata": {},
   "source": [
    "## 5.1 일관성 있는 텍스트 생성의 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2b998",
   "metadata": {},
   "source": [
    "지금까지 사전 훈련과 지도 학습 기반의 미세 튜닝을 조합해 NLP 문제를 다루는 데 초점을 둡니다. 시퀀스나 토큰 분류 같이 작업에 특화된 헤드에서 예측 생성은 매우 간단합니다. 일련의 로짓을 출력하고 최댓값을 선택하여 예측 클래스를 얻습니다. 또는 소프트맥스 함수를 적용해 클래스별 예측 확률을 얻습니다. 이와 달리 모델의 확률 출력을 텍스트로 변환하려면 **디코딩 방법(decoding method)** 이 필요합니다. 여기에는 텍스트 생성에만 따르는 특수한 어려움이 있습니다.\n",
    "\n",
    "  - 디코딩은 반복적으로 수행되므로 입력이 모델의 정방향 패스를 한 번 통과할 때보다 많은 계산이 필요함\n",
    "  \n",
    "  - 생성된 텍스트의 품질과 다양성은 디코딩 방법과 이에 관련된 하이퍼파라미터에 따라 달라짐\n",
    "  \n",
    "디코딩이 어떻게 수행되는지 이해하기 위해 먼저 GPT-2의 사전 훈련 방법과 텍스트 생성에 적용하는 과정을 알아봄\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0456d001",
   "metadata": {},
   "source": [
    "다른 **자기회귀 모델**(autoregressive model) 또는 **코잘 언어 모델**(causal language model)과 마찬가지로, GPT-2는 시작 프롬프트 또는 문맥 시퀀스 $ x = x_1, x_2, ... , x_k$ 가 주어질 때 텍스트에 등장하는 토큰 시퀀스 $y = y_1, y_2, ..., y_t $의 확률 $P(y|x)$를 추정하도록 사전 훈련됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ab660",
   "metadata": {},
   "source": [
    "직접 $P(y|x)$를 추정하기 위해 충분한 훈련 데이터를 획득하기란 불가능하므로, 일반적으로 확률의 연쇄 법칙(chain rule)을 사용해 조건부 확률(conditional problem)의 곱으로 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7efeb",
   "metadata": {},
   "source": [
    "$$P(y_1, ... , y_t | x) = \\prod_{t=1}^{n}P(y_t | y_{<t}, x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fbe457",
   "metadata": {},
   "source": [
    "여기서 $y_{<t}$는 시퀀스 $y_1, ... , y_{t-1}$을 간략화한 식입니다. 이 조건부 확률로 자기 회귀 언어 모델링은 문장의 이전 단어가 주어지면 다음 단어를 예측한다는 직관을 얻을 수 있습니다. 앞선 식의 오른쪽 항에 위치한 확률이 이를 설명합니다. 이런 사전 훈련 목표는 과거와 미래의 문맥을 모두 사용해 마스킹된 토큰을 예측하는 BERT와 매우 다릅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62962fe",
   "metadata": {},
   "source": [
    "이제 다음 토큰 예측 작업이 임의의 길이를 가진 텍스트 시퀀스를 생성할 때 어떻게 적용할 지 예상됩니다. 아래 그림처럼 \"Transformers are the\"와 같은 프롬프트로 시작하면 모델은 다음 토큰을 예측합니다. 다음 토큰이 결정되면 이를 프롬프트에 추가해 새로운 입려 시퀀스를 만들고 또 다른 토큰을 생성합니다. 이 과정을 특수한 시퀀스 종료 토큰이나 사전에 정의한 최대 길이에 도달할 때까지 반복합니다.\n",
    "\n",
    "![그림5-3](image/chapter05_text-generation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081e44c",
   "metadata": {},
   "source": [
    "**NOTE** 출력 시퀀스가 입력 프롬프트에 따라 결정되므로 이런 종류의 텍스트 생성을 종종 **조건부 텍스트 생성(conditional text generation)** 이라 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ce478f",
   "metadata": {},
   "source": [
    "이 과정의 핵심은 각 타임스텝에서 어떤 토큰을 선택할지 결정하는 디코딩 방법에 있습니다. 언어 모델의 헤드는 각 스텝에서 어휘사전에 있는 토큰마다 로짓 $z_{t, i}$을 생성하므로 소프트맥스를 적용하면 가능한 다음 토큰 $w_i$에 대한 확률 분포를 얻습니다. \n",
    "\n",
    "$$ P(y_t = w_i | y_{<t}, x) = softmax(z_{t, i}) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dae65c",
   "metadata": {},
   "source": [
    "대부분의 디코딩 방법은 다음과 같은 $\\hat{y}$를 선택해 전체적으로 확률이 가장 높은 시퀀스를 찾습니다.\n",
    "\n",
    "$$\\hat{y} = \\underset{y}{argmax}P(y|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981d583",
   "metadata": {},
   "source": [
    "직접 $\\hat{y}$를 찾으려면 언어 모델로 가능한 모든 시퀀스를 평가해야 합니다. 이런 작업을 합리적인 시간 안에 할수 있는 알고리즘이 없으므로 근사적인 방법을 사용합니다. 이 장에서 이런 근사적인 방법 몇가지를 알아보고 고품질 텍스트를 생성하는 더 똑똑하고 복잡한 알고리즘을 점진적으로 구축하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bfe4e",
   "metadata": {},
   "source": [
    "## 5.2 그리디 서치 디코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f136cbc",
   "metadata": {},
   "source": [
    "연속적인 모델 출력에서 이산적인 토큰을 얻는 가장 간단한 디코딩 방법은 각 타임 스텝에서 확률이 가장 높은 토큰을 탐욕적(greedily)으로 선택하는 것입니다. \n",
    "\n",
    "$$ \\hat{y_{t}} = \\underset{y_t}{argmax}P(y_t|y_{<t}, x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878eaf0",
   "metadata": {},
   "source": [
    "그리디 서치 방법을 알아보기 위해 언어 모델링 헤더를 가진 15억개 파라미터의 GPT-2 버전을 로드함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024df04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 22:38:48.130765: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-29 22:38:48.329606: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-29 22:38:48.895664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ebacba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 29 22:38:52 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.105.01   Driver Version: 515.105.01   CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   40C    P2    47W / 170W |   1711MiB / 12288MiB |      7%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      2297      G   /usr/lib/xorg/Xorg                285MiB |\r\n",
      "|    0   N/A  N/A      2434      G   /usr/bin/gnome-shell               27MiB |\r\n",
      "|    0   N/A  N/A     26387      G   ...863751312754158865,262144      146MiB |\r\n",
      "|    0   N/A  N/A     28297      C   ...3/envs/py31011/bin/python     1227MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7286c92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 487.47 MB\n",
      "Cached Memory: 542.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# GPU 메모리 사용량 확인\n",
    "if device.type == \"cuda\":\n",
    "    allocated_memory = torch.cuda.memory_allocated(device=device)\n",
    "    cached_memory = torch.cuda.memory_reserved(device=device)\n",
    "    print(f\"Allocated Memory: {allocated_memory / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"Cached Memory: {cached_memory / 1024 ** 2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33157e59",
   "metadata": {},
   "source": [
    "그럼 텍스트를 생성함. 허깅페이스 트랜스포머스는 GPT-2 같은 자기회귀 모델을 위해 generatre() 함수를 제공하지만, 작동 방식을 이해하기 위해 직접 이 디코딩 메서드를 구현하겠습니다. 연습 삼아 [그림 5-3]에 있는 반복적인 과정을 그대로 따르겠습니다. \"Transformers are the\"를 입력 프롬프트로 사용하고 여덟 번의 타임스텝 동안 디코딩을 수행합니다. 각 타임스텝에서 프롬프트의 마지막 토큰에 대한 로짓을 선택하고 소프트맥스를 적용해 확률 분포를 얻습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966df218",
   "metadata": {},
   "source": [
    "그 다음 확률이 가장 높은 토큰을 다음 토큰으로 선택하고 입력 시퀀스에 추가한 후에 이 과정을 다시 반복합니다. 또 대안을 시각적으로 보여주기 위해 타임 스텝마다 확률이 가장 높은 토큰을 다섯개 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ae3b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (9.76%)</td>\n",
       "      <td>same (2.94%)</td>\n",
       "      <td>only (2.87%)</td>\n",
       "      <td>best (2.38%)</td>\n",
       "      <td>first (1.77%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>common (22.90%)</td>\n",
       "      <td>powerful (6.88%)</td>\n",
       "      <td>important (6.32%)</td>\n",
       "      <td>popular (3.95%)</td>\n",
       "      <td>commonly (2.14%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most common</td>\n",
       "      <td>type (15.06%)</td>\n",
       "      <td>types (3.31%)</td>\n",
       "      <td>form (1.91%)</td>\n",
       "      <td>way (1.89%)</td>\n",
       "      <td>and (1.49%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most common type</td>\n",
       "      <td>of (83.13%)</td>\n",
       "      <td>in (3.16%)</td>\n",
       "      <td>. (1.92%)</td>\n",
       "      <td>, (1.63%)</td>\n",
       "      <td>for (0.88%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most common type of</td>\n",
       "      <td>particle (1.55%)</td>\n",
       "      <td>object (1.02%)</td>\n",
       "      <td>light (0.71%)</td>\n",
       "      <td>energy (0.67%)</td>\n",
       "      <td>objects (0.66%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most common type of particle</td>\n",
       "      <td>. (14.26%)</td>\n",
       "      <td>in (11.57%)</td>\n",
       "      <td>that (10.19%)</td>\n",
       "      <td>, (9.57%)</td>\n",
       "      <td>accelerator (5.81%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most common type of parti...</td>\n",
       "      <td>They (17.48%)</td>\n",
       "      <td>\\n (15.19%)</td>\n",
       "      <td>The (7.06%)</td>\n",
       "      <td>These (3.09%)</td>\n",
       "      <td>In (3.07%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most common type of parti...</td>\n",
       "      <td>are (38.78%)</td>\n",
       "      <td>have (8.14%)</td>\n",
       "      <td>can (7.98%)</td>\n",
       "      <td>'re (5.04%)</td>\n",
       "      <td>consist (1.57%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input           Choice 1   \n",
       "0                               Transformers are the       most (9.76%)  \\\n",
       "1                          Transformers are the most    common (22.90%)   \n",
       "2                   Transformers are the most common      type (15.06%)   \n",
       "3              Transformers are the most common type        of (83.13%)   \n",
       "4           Transformers are the most common type of   particle (1.55%)   \n",
       "5  Transformers are the most common type of particle         . (14.26%)   \n",
       "6  Transformers are the most common type of parti...      They (17.48%)   \n",
       "7  Transformers are the most common type of parti...       are (38.78%)   \n",
       "\n",
       "            Choice 2            Choice 3          Choice 4   \n",
       "0       same (2.94%)        only (2.87%)      best (2.38%)  \\\n",
       "1   powerful (6.88%)   important (6.32%)   popular (3.95%)   \n",
       "2      types (3.31%)        form (1.91%)       way (1.89%)   \n",
       "3         in (3.16%)           . (1.92%)         , (1.63%)   \n",
       "4     object (1.02%)       light (0.71%)    energy (0.67%)   \n",
       "5        in (11.57%)       that (10.19%)         , (9.57%)   \n",
       "6        \\n (15.19%)         The (7.06%)     These (3.09%)   \n",
       "7       have (8.14%)         can (7.98%)       're (5.04%)   \n",
       "\n",
       "               Choice 5  \n",
       "0         first (1.77%)  \n",
       "1      commonly (2.14%)  \n",
       "2           and (1.49%)  \n",
       "3           for (0.88%)  \n",
       "4       objects (0.66%)  \n",
       "5   accelerator (5.81%)  \n",
       "6            In (3.07%)  \n",
       "7       consist (1.57%)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors = \"pt\")[\"input_ids\"].to(device) # return_tensors가 파이토치의 텐서 형태로 반환\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids = input_ids)\n",
    "        \n",
    "        # 첫 번째 배치의 마지막 토큰의 로짓을 선택해 소프트맥스를 적용합니다.\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim = -1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim = -1, descending = True)\n",
    "        \n",
    "        # 가장 높은 확률의 토큰을 저장\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100*token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx + 1}\"] = token_choice \n",
    "        \n",
    "        # 예측한 다음 토큰을 입력에 추가합니다. \n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim = -1)\n",
    "        iterations.append(iteration)\n",
    "    \n",
    "pd.DataFrame(iterations)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc864db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input': 'Transformers are the most common type of particle. They',\n",
       " 'Choice 1': ' are (38.78%)',\n",
       " 'Choice 2': ' have (8.14%)',\n",
       " 'Choice 3': ' can (7.98%)',\n",
       " 'Choice 4': \"'re (5.04%)\",\n",
       " 'Choice 5': ' consist (1.57%)'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations[7] # gpt2라서 책과 결과가 사뭇 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bc339",
   "metadata": {},
   "source": [
    "각 스텝에서 가능한 다른 문장도 볼 수 있는데, 이는 텍스트 생성의 반복적인 특성을 보여줌. 예측하는데 한 번의 정방향 패스로 충분한 시퀀스 분류 등의 작업과 달리 텍스트 생성은 한 번에 하나의 출력 토큰을 디코딩합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1788d",
   "metadata": {},
   "source": [
    "그리드 서치 구현은 어렵지 않지만 더 복잡한 디코딩 방법을 알아보기 위해 허깅페이스 트랜스포머스에 내장된 generate() 함수를 사용.\n",
    "앞선 결과를 재현하기 위해 샘플링을 끄고(체크포인트에서 로딩한 모델 설정에 따로 지정되지 않았다면 이 옵션은 기본적으로 False입니다.) 생성 토큰의 개수를 max_new_tokens 매개변수로 지정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2edc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most common type of particle. They are\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens = n_steps, do_sample = False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c9d02",
   "metadata": {},
   "source": [
    "**에러 내용**: \"어텐션 마스크와 패드 토큰 ID가 설정되지 않았습니다. 그 결과, 예상치 못한 동작을 관찰할 수 있습니다. 안정적인 결과를 얻기 위해 입력의 attention_mask를 전달해 주세요. 열린 끝 생성을 위해 pad_token_id를 eos_token_id:50256로 설정합니다.\"\n",
    "\n",
    "\n",
    "\n",
    "이 메시지는 Hugging Face의 Transformers 라이브러리를 사용할 때, 주의해야 하는 사항을 알려주고 있습니다.\n",
    "\n",
    "1. attention_mask가 설정되지 않았다는 부분: attention_mask는 특정 토큰이 모델에 의해 주의를 받아야 하는지(즉, 토큰이 중요한 정보를 포함하고 있는지) 나타내는 값을 의미합니다. 이 값이 설정되지 않으면, 모델은 토큰 간의 관계를 제대로 파악하지 못할 수 있습니다. 따라서 입력의 attention_mask를 설정하여 안정적인 결과를 얻는 것이 중요합니다.\n",
    "\n",
    "2. pad_token_id가 설정되지 않았다는 부분: 패딩 토큰은 일반적으로 여러 입력을 동일한 길이로 만들기 위해 사용됩니다. 따라서 pad_token_id를 eos_token_id로 설정한다는 것은 문장의 끝을 나타내는 eos_token_id를 패딩 토큰으로 사용하겠다는 의미입니다. 이것은 열린 끝 생성 (open-end generation)에 대한 기본 설정입니다.\n",
    "\n",
    "만약 위의 사항을 수정하려면, 당신의 코드에 다음과 같이 attention_mask와 pad_token_id를 명시적으로 설정해주면 됩니다.\n",
    "\n",
    "```python\n",
    "# 수정 코드 예시 \n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)[\"input_ids\"].to(device)\n",
    "attention_mask = tokenizer(input_txt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)[\"attention_mask\"].to(device)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142ed45",
   "metadata": {},
   "source": [
    "이제 조금 더 재미있는 시도를 실시. OpenAI의 유니콘 기사 재현\n",
    "\n",
    "앞에서 처럼 토크나이저로 프롬프트를 인코딩하고 긴 텍스트 시퀀스를 생성하기 위해 max_length에 큰 값을 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f3d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, \\\n",
    "in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d01836b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors = \"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length = max_length, do_sample = False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf92b2",
   "metadata": {},
   "source": [
    "그리드 서치 알고리즘은 반복적인 출력 시퀀스를 생성하는 경향이 있어서 뉴스 기사로는 확실히 적절하지 않음.\n",
    "\n",
    "이는 그리드 서치 알고리즘의 보편적인 문제이며, 이로 인해 최적의 솔루션을 만들어내기 어렵습니다. \n",
    "\n",
    "디코딩 측면에서 보면, 확률이 높은 단어가 확률이 낮은 단어보다 먼저 등장하기 때문에 전체적으로 확률이 높은 단어 시퀀스를 생성하지 못하기도 합니다. \n",
    "\n",
    "다행히 더 나은 방법이 있습니다. **빔 서치 디코딩(beam search decoding)** 이라는 인기 있는 방법을 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e77926",
   "metadata": {},
   "source": [
    "**(Note)** 그리디 서치 디코딩은 다양성이 필요한 텍스트 생성 작업에는 거의 사용되지 않지만, 결정적이고 사실적으로 정확한 출력이 필요한 수식 등의 짧은 문장 생성에는 유용합니다. 이런 작업을 위해 줄바꿈이 있는 \"5 + 8 => 13 \\n 7 + 2 => 9 \\n 1 + 0 =>\" 같은 형식의 입력 프롬프트를 제공해 GPT-2의 조건부 생성을 제어할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48b604",
   "metadata": {},
   "source": [
    "## 5.3 빔 서치 디코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c01a17",
   "metadata": {},
   "source": [
    "빔 서치는 각 스텝에서 확률이 가장 높은 토큰을 디코딩하는 대신, 확률이 가장 높은 상위 b개의 다음 토큰을 추적합니다. \n",
    "\n",
    "여기서 b는 **빔(beam)** 또는 **불완전 가설(partial hypothesis)** 의 개수입니다. \n",
    "\n",
    "다음 빔 세트는 기존 세트에서 가능한 모든 다음 토큰을 확장하고 확률이 가장 높은 b개의 확장을 선택해 구성합니다. \n",
    "\n",
    "이 과정은 최대 길이나 EOS 토큰에 도달할 때까지 반복됩니다. \n",
    "\n",
    "  - 확률이 가장 높은 시퀀스는 로그 확률에 따라 b개 빔의 순위를 매겨 선택됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71914b7",
   "metadata": {},
   "source": [
    "**< b=2인 빔 서치 >** \n",
    "\n",
    "![그림 5-4](image/chapter05_beam-search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac05c9",
   "metadata": {},
   "source": [
    "왜 확률이 아니라 로그 확률을 사용해 시퀀스 점수를 매길까요? \n",
    "\n",
    "시퀀스의 전체 확률 $P(y_1, y_2, ... , y_t|x)$을 계산하려면 조건부 확률 $P(y_t|y_{<t}, x)$의 곱을 계산해야 하기 때문\n",
    "\n",
    "각 조건부 확률이 일반적으로 [0, 1] 범위 안에 속한 작은 값이므로 이를 곱해 얻은 전체 확률은 언더플로(underflow)가 쉽게 발생합니다. \n",
    "\n",
    "  - 컴퓨터가 이 계산의 결과를 더 이상 정확하게 표현할 수 없다는 의미!\n",
    "  \n",
    "  - 예를 들어 t=1024개의 토큰으로 이루어진 시퀀스에서 각 토큰의 확률이 0.5라고 하면 시퀀스 전체 확률은 매우 작은 수가 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a199391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.562684646268003e-309"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 ** 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c0484",
   "metadata": {},
   "source": [
    "이런 값은 수치적으로 불안정해 언더플로가 발생하지만, 로그 확률을 계산하면 이를 피할 수 있음\n",
    "\n",
    "결합 확률(joint probability)과 조건부 확률(conditional probability)에 로그를 적용하면 로그의 곱셉 규칙에 따라 다음과 같은 식이 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1810b9",
   "metadata": {},
   "source": [
    "$$logP(y_1, ..., y_t|x) = \\sum_{t=1}^{N}logP(y_t|y_{<t}, x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3132498",
   "metadata": {},
   "source": [
    "다른 말로 하면, 앞서 본 확률의 곱셈이 로그 확률의 덧셈으로 바뀜\n",
    "\n",
    "이 방식이 수치적 불안정을 일으킬 확률이 훨씬 적습니다. 예를 들어 이전 예에 대한 로그 확률은 다음과 같이 계산함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fddfc898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-709.7827128933695"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "sum([np.log(0.5)]*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a8496",
   "metadata": {},
   "source": [
    "이런 값이 더 다루기 쉬울 뿐 아니라 이 방식은 더 작은 수에도 적용됩니다. 상대적 확률만 비교하면 되므로 로그 확률을 사용해서도 비교가 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1b4be",
   "metadata": {},
   "source": [
    "이제 그리디 서치와 빔 서치로 생성한 텍스트의 로그 확률을 계산해 빔 서치가 전체 확률을 향상하는지 확인하겠습니다. 허깅페이스 트랜스포머스 모델은 입력 토큰이 주어지면 다음 토큰에 대한 정규화되지 않은 로짓을 반환합니다. 따라서 먼저 로짓을 정규화해서 시퀀스의 각 토큰을 위해 전체 어휘사전에 대한 확률 분포를 만듭니다. 그 다음 시퀀스에 있는 토큰 확률만 선택합니다. 이 단계를 구현한 함수는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784006d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim = -1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13233e3",
   "metadata": {},
   "source": [
    "이 함수는 하나의 토큰에 대한 로그 확률을 제공하므로, 시퀀스의 전체 로그 확률을 얻으려면 각 토큰의 로그 확률을 더합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a790fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len = 0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[: , 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a4381",
   "metadata": {},
   "source": [
    "모델이 입력 시퀀스를 생성하지 않았으므로 입력 시퀀스의 로그 확률은 무시합니다. 또 로짓과 레이블의 정렬이 중요합니다. 모델은 다음 토큰을 예측하기 때문에 첫 번째 레이블에 대한 로짓을 얻지 못합니다. 또 마지막 로짓에 대한 정답이 없기 때문에 마지막 로짓은 필요하지 않습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850375c",
   "metadata": {},
   "source": [
    "이 함수를 사용해 OpenAI 프롬프트에서 그리디 서치로 만든 시퀀스의 로그 확률을 계산하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d28c6aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very\n",
      "\n",
      "로그 확률: -83.33\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len = len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\n로그 확률: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87e1ea",
   "metadata": {},
   "source": [
    "이를 빔 서치로 생성한 시퀀스와 비교해보겠습니다. generate() 함수에서 빔 서치를 활성화하려면 num_beams 매개변수에 빔 개수를 지정합니다. 빔 크기가 클수록 결과가 더 좋을 가능성이 높습니다. 하지만 각 빔에 대해 병렬적으로 시퀀스를 생성하므로 생성 과정이 훨씬 느려집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9741bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, San Diego, and the University of California, Santa Cruz, found that the unicorns were able to communicate with each other in a way that was similar to that of human speech.\n",
      "\n",
      "\n",
      "\"The unicorns were able to communicate with each other in a way that was similar to that of human speech,\" said study co-lead author Dr. David J.\n",
      "\n",
      "로그 확률: -78.340492\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length = max_length, num_beams = 5, do_sample = False)\n",
    "logp = sequence_logprob(model, output_beam, input_len = len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\n로그 확률: {logp:2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597572bc",
   "metadata": {},
   "source": [
    "단순한 그리디 서치보다 빔 서치에서 더 높은 로그 확률을 얻었습니다.(높을수록 좋습니다.)\n",
    "\n",
    "하지만 빔 서치도 텍스트가 반복되는 문제가 있습니다. \n",
    "\n",
    "이 문제를 해결하기 위해 no_repeat_ngram_size 매개변수로 n-그램 페널티를 부과하는 방법이 있습니다. \n",
    "\n",
    "그러면 지금까지 n-그램을 추적해 이전에 보았던 n-그램을 생성하는 경우 다음 토큰 확률이 0이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5110e9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, San Diego, and the National Science Foundation (NSF) in Boulder, Colorado, were able to translate the words of the unicorn into English, which they then translated into Spanish.\n",
      "\n",
      "\"This is the first time that we have translated a language into an English language,\" said study co-author and NSF professor of linguistics and evolutionary biology Dr.\n",
      "\n",
      "로그 확률: -101.88\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, \n",
    "                             max_length = max_length, \n",
    "                             num_beams = 5, \n",
    "                             do_sample = False,\n",
    "                             no_repeat_ngram_size = 2)\n",
    "\n",
    "logp = sequence_logprob(model, output_beam, input_len = len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\n로그 확률: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c88a52",
   "metadata": {},
   "source": [
    "결과가 그리 나쁘지 않습니다. 반복적인 텍스트 생성을 막고 점수는 더 낮아졌지만 텍스트가 일관성을 유지함\n",
    "\n",
    "n-그램 패널티를 사용한 빔 서치는 확률이 높은 토큰에 초점을 맞추는 빔 서치와 반복을 줄이는 n-그램 패널티의 균형을 잡는 좋은 방법입니다. \n",
    "\n",
    "사실적인 정확성을 요하는 요약, 기계 번역 같은 애플리케이션에 널리 사용됨. \n",
    "\n",
    "또 분야에 국한되지 않는 잡담이나 기사처럼 사실적인 정확성이 다양성보다 덜 중요할 때, 샘플링을 사용해 다양성을 늘리면서 반복을 줄이는 방법도 있습니다. \n",
    "\n",
    "많이 사용하는 샘플링 방법 몇 가지를 조사하면서 텍스트 생성에 대한 탐구를 마무리하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a53894",
   "metadata": {},
   "source": [
    "## 5.4 샘플링 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5064d",
   "metadata": {},
   "source": [
    "가장 간단한 샘플링 방법은 각 타임스텝 내에 모델이 출력한 전체 어휘 사전의 확률 분포에서 랜덤하게 샘플링하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434d25b",
   "metadata": {},
   "source": [
    "$$ P(y_t = w_i | y_{<t}, x) = softmax(z_{t, i}) = \\frac{exp(z_{t, i})}{\\sum_{\\left | V \\right |}^{j=1}exp(z_{t, j})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ab096",
   "metadata": {},
   "source": [
    "여기서 $\\left | V \\right |$ 는 어휘사전의 크기(cardinality)를 나타냅니다. 소프트맥스 함수를 적용하기 전에 로짓의 스케일을 조정하는 온도 파라미터 T를 추가하면 출력의 다양성이 쉽게 제어됩니다.\n",
    "\n",
    "$$ P(y_t = w_i | y_{<t}, x)  = \\frac{exp(z_{t, i}/T)}{\\sum_{\\left | V \\right |}^{j=1}exp(z_{t, j}/T)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7866ac",
   "metadata": {},
   "source": [
    "T 값을 바꾸면 확률 분포의 형태가 제어됩니다.(물리학을 조금 안다면 눈치챘겠지만 볼츠만 분포와 매우 비슷)\n",
    "\n",
    "T << 1일 때 이 분포는 원점 근처에서 정점에 도달하고 드문 토큰을 억제합니다.\n",
    "\n",
    "반면 T >> 1 일 때 분포가 평평해지고 각 토큰의 확률이 동일해집니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddd3038",
   "metadata": {},
   "source": [
    "온도가 생성되는 텍스트에 어떤 영향을 미치는지 알아보기 위해 generate() 함수의 temperature 매개변수를 T=2로 지정해 샘플링 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80c974ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "Tang -------- en Epic Yukpresaailiated y Amon reused thief lakh brokenPurSept Wednesday Result Deep Sith trade Contribution transfer Da queen Shut McCartney props Chow costason licenses Theatre should47 Standing inscription rejection flags mog burst percentarling dictatorsTempaking choir realGerman neighbours overflow Raj Pok Ik Diet extremism excitement taik testified blonde Writers great Jazz Imp interventions stare burn Saddam'? crawled jewels Related all�resmed away Foreign demand\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length = max_length, do_sample = True, \n",
    "                            temperature = 2.0, top_k = 0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424fbe07",
   "metadata": {},
   "source": [
    "온도가 높으면 횡설수설에 가까운 텍스트가 생성됨.\n",
    "\n",
    "드문 토큰이 강조되어 모델이 이상한 문법을 만들고 다양한 가짜 단어를 만들어 냈습니다.\n",
    "\n",
    "온도를 낮추면 어던 일이 일어나는지 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86f5f836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers found that the unicorns were \"very closely related\" to the human and were \"very intelligent\" in their language.\n",
      "\n",
      "The unicorns also had a large brain, which they developed to help them read and write.\n",
      "\n",
      "\"We think that the unicorns were very closely related to humans and were very intelligent in their language,\" says Michael Wessels, a professor in the department\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length = max_length, do_sample = True, \n",
    "                            temperature = 0.5, top_k = 0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f1b1e5",
   "metadata": {},
   "source": [
    "온도는 샘플의 품질을 제어하지만, 항상 일관성(낮은 온도)과 다양성(높은 온도)의 균형점이 있기 때문에 당면한 문제에 따라 조정해야 함\n",
    "\n",
    "일관성과 다양성의 균형을 조정하는 또 다른 방법으로 어휘사전의 분포를 잘라내는 방법이 있습니다.\n",
    "\n",
    "이 방법은 온도와 함께 다양성을 자유롭게 조정하지만, 더 제한된 범위에서 문맥상 매우 이상한 단어(즉, 확률이 낮은 단어)를 제외합니다. \n",
    "\n",
    "대표적인 방법은 탑-k 샘플링과 뉴클리어스 샘플링(nucleus sampling)(또는 탑-p 샘플링)입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81164ede",
   "metadata": {},
   "source": [
    "## 5.5 탑-k 및 뉴클리어스 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65193074",
   "metadata": {},
   "source": [
    "탑-k와 뉴클리어스 (탑-p) 샘플링은 온도를 사용하는 방법에 대해 잘 알려진 두 가지 대안 또는 확장입니다. 두 샘플링은 모두 각 타임스텝에서 샘플링에 사용할 토큰의 개수를 줄인다는 개념에 기초합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c85da",
   "metadata": {},
   "source": [
    "탑-k 샘플링은 확률이 가장 높은 k개 토큰에서만 샘플링해서 확률이 낮은 토큰을 피합니다. 이렇게 하면 확률 분포의 롱테일(long-tail)을 잘라내고 확률이 가장 높은 토큰에서만 샘플링하는 것이 가능해집니다. generate() 함수는 이 작업을 쉽게 수행하는 매개변수 top_k를 제공합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf650915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The unicorns are thought to have evolved from a single species, the horned echidna. The horned echidna is the only species that has ever been found in the Andes, and it's likely that they also have a peculiar way of speaking. The researchers say that they were able to decipher the language of the unicorn by looking at their eyes.\n",
      "\n",
      "\n",
      "The researchers say that they\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length = max_length, do_sample = True,\n",
    "                            temperature = 0.5, top_k = 0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915708e3",
   "metadata": {},
   "source": [
    "k 값은 수동으로 선택야하고 실제 출력 분포에 상관없이 시퀀스의 각 선택에 동일하게 적용됩니다. 다음 장에서 살펴볼 몇 가지 텍스트 품질 지표를 사용해 적당한 k 값을 찾겠습니다. 하짐나 고정된 컷오프(cut-off)는 만족스럽지 않은 경우도 있습니다. \n",
    "\n",
    "이에 대한 대안으로 동적인 컷오프를 적용하는 방법이 있습니다. 뉴클리어스 샘플링 또는 탑-p 샘플리에서는 고정된 컷오프 값을 선택하지 않고 어디서 컷오프를 할지 조건을 지정합니다. 이 조건은 선택한 특정 확률 질량(probability mass)에 도달할 때입니다. 이 값을 95%로 지정했다고 해보죠. 그 다음 확률에 따라 내림차순으로 모든 토큰을 정렬하고 선택한 토큰의 확률 값이 95%에 도달할 때까지 이 리스트의 맨 위부터 토큰을 하나씩 추가합니다. 출력 분포에 따라(확률이 매우 높은) 하나의 토큰이 될 수돌 있고 (확률이 비슷한) 백 개의 토큰이 될 수도 있습니다. 아마 이제는 generate() 함수가 탑-p 샘플링을 위한 매개변수도 제공한다는 사실이 놀랍지 않을 겁니다. 한 번 적용해 보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef7967e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "Their \"English\" was derived from the Latin \"hie\", meaning \"little.\" The \"English\" is not a phonetic alphabet, but a simple set of simple letters called hé (pronounced like \"he\"). Each letter is written on the back of a cactus, and has four different meanings: English, Spanish, German, Spanish and Russian.\n",
      "\n",
      "\n",
      "The findings, published in the\n"
     ]
    }
   ],
   "source": [
    "output_topp = model.generate(input_ids, max_length = max_length, do_sample = True,\n",
    "                            top_p = 0.90)\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e84d34",
   "metadata": {},
   "source": [
    "탑-p 샘플링도 일관성 있는 텍스트를 생성합니다.\n",
    "\n",
    "두 샘플링 방법을 연결하면 양쪽의 장점을 모두 취할 수 있습니다. \n",
    "\n",
    "top_k = 50와 top_p=0.9로 지정하면 확률이 가장 높은 50개 토큰에서 확률 질량이 90%인 토큰을 선택하게 됩니다. \n",
    "\n",
    "**Note** 샘플링할 때 빔 서치를 적용하는 방법도 있습니다. 다음 후보 토큰의 배치를 탐욕적으로 선택하지 않고 샘플링을 통해 같은 식으로 빔 세트를 구성하는 방법입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c8445",
   "metadata": {},
   "source": [
    "## 5.6 어떤 디코딩 방법이 최선일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35763b39",
   "metadata": {},
   "source": [
    "안타깝지만 언제나 통하는 최선의 디코딩 방법은 없습니다. 최선의 방법은 주어진 텍스트 생성 작업의 특성에 따라 다릅니다. 수식이나 특정 질문에 답을 내듯 정밀한 작업을 수행하는 모델이라면 온도를 낮추거나 확률이 가장 높은 답을 보장하기 위해 빔 서치와 함께 그리디 서치 같은 결정론적인 방법을 사용합니다. 모델이 더 길고 창의적인 텍스트를 생성하려면 샘플링 방법으로 바꾸고 온도를 올리거나 탑-k와 뉴클리어스 샘플링을 혼합해 사용하는 편이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c83ec",
   "metadata": {},
   "source": [
    "## 5.7 결론 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d523e6",
   "metadata": {},
   "source": [
    "이 장에서 이전에 본 NLU(Natural Language Understanding) 작업과 매우 다른 작업인 텍스트 생성을 살펴보았습니다. \n",
    "\n",
    "텍스트를 생성하려면 토큰마다 적어도 한 번의 정방향 패스가 필요합니다. 빔 서치를 사용한다면 더 많이 필요합니다.\n",
    "\n",
    "따라서 텍스트 생성 시 계산량에 대한 요구가 많고 대규모로 텍스트 생성 모델을 실행하려면 적절한 인프라가 필요합니다. \n",
    "\n",
    "또 모델의 출력 확률을 이산적인 토큰으로 변환하는 좋은 디코딩 전략이 텍스트 품질을 향상시켜주기도 합니다. \n",
    "\n",
    "최선의 디코딩 전략을 찾으려면 약간의 실험과 생성된 텍스트에 대한 주관적인 평가가 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4a39c",
   "metadata": {},
   "source": [
    "하지만 실전에서는 직감만으로 이런 결정을 내리지 않습니다! 다른 NLP 작업과 마찬가지로 해결하려는 문제를 반영한 모델 성능 지표를 선택합니다.\n",
    "\n",
    "당연히 다양한 선택지가 있으며, 다음 장에서 텍스트 요약 모델을 훈련하고 평가하는 방법을 살펴보면서 가장 널리 사용되는 지표도 알아보겠습니다. \n",
    "\n",
    "GPT 유형의 모델을 밑바닥부터 훈련하는 방법이 궁금하다면 곧장 10장으로 넘어가도 좋습니다.\n",
    "(10장에서는 대규모 코드 데이터 셋을 수집해 자기회귀 언어 모델(autoregressive language model)을 훈련합니다.)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
